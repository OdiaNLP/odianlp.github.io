{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"There has been few to less content on Natural Language Processing for low resource languages like Odia. This is an effort by few volunteers to increase the NLP content over Internet.","title":"Home"},{"location":"contributions/","text":"All the contribution are Open source and freely available (with proper attribution) to the society. OdiaNLP has done either entire or partial contributions to the following projects: Mozilla Common Voice # Speech corpora creation through Mozilla Common Voice. 150MB Speech data has been prepared with purely volunteering efforts. Google Translation API wrapper # Odia has been added into Unofficial Google Translation API wrapper. $ pip install googletrans >>> from googletrans import Translator >>> translator = Translator() >>> translator.translate(\"Hello Odia people\", dest=\"or\").text # '\u0b28\u0b2e\u0b38\u0b4d\u0b15\u0b3e\u0b30 \u0b13\u0b21\u0b3f\u0b06 \u0b32\u0b4b\u0b15\u0b2e\u0b3e\u0b28\u0b47 |' Fake Odia name generation # For fake name generation purposes Odia support has been added to the best data anonymization library, Faker. $ pip install Faker >>> from faker import Faker >>> fake = Faker(\"or_IN\") >>> for _ in range(10): ... print(fake.name()) ... \u0b1a\u0b3f\u0b24\u0b30\u0b02\u0b1c\u0b28 \u0b28\u0b28\u0b4d\u0b26\u0b3f \u0b30\u0b3e\u0b1c, \u0b30\u0b2c\u0b3f\u0b28\u0b3e\u0b30\u0b3e\u0b5f\u0b23 \u0b15\u0b47\u0b26\u0b3e\u0b30\u0b28\u0b3e\u0b25 \u0b2c\u0b30\u0b4d\u0b2e\u0b3e \u0b05\u0b2e\u0b30\u0b28\u0b3e\u0b25 \u0b38\u0b47\u0b20\u0b40 \u0b38\u0b3e\u0b32\u0b41\u0b1c\u0b3e, \u0b15\u0b33\u0b4d\u0b2a\u0b24\u0b30\u0b41 \u0b26\u0b47\u0b2c\u0b30\u0b3e\u0b1c \u0b30\u0b3e\u0b27\u0b3e\u0b30\u0b3e\u0b23\u0b40 \u0b2a\u0b4b\u0b26\u0b4d\u0b26\u0b3e\u0b30 \u0b30\u0b3e\u0b27\u0b41 \u0b2e\u0b24\u0b32\u0b41\u0b2c \u0b36\u0b24\u0b2a\u0b25\u0b40 \u0b30\u0b28\u0b4d\u0b27\u0b3e\u0b30\u0b40, \u0b38\u0b41\u0b36\u0b3e\u0b28\u0b4d\u0b24 \u0b17\u0b48\u0b3e\u0b24\u0b2e \u0b13\u0b30\u0b3e\u0b2e Localization projects # Various localization projects to make websites available in Odia language Telegram # Mozilla Firefox (In-Progress) # Duckduckgo # COVID-19 website (Unofficial) #","title":"Contributions"},{"location":"contributions/#mozilla-common-voice","text":"Speech corpora creation through Mozilla Common Voice. 150MB Speech data has been prepared with purely volunteering efforts.","title":"Mozilla Common Voice"},{"location":"contributions/#google-translation-api-wrapper","text":"Odia has been added into Unofficial Google Translation API wrapper. $ pip install googletrans >>> from googletrans import Translator >>> translator = Translator() >>> translator.translate(\"Hello Odia people\", dest=\"or\").text # '\u0b28\u0b2e\u0b38\u0b4d\u0b15\u0b3e\u0b30 \u0b13\u0b21\u0b3f\u0b06 \u0b32\u0b4b\u0b15\u0b2e\u0b3e\u0b28\u0b47 |'","title":"Google Translation API wrapper"},{"location":"contributions/#fake-odia-name-generation","text":"For fake name generation purposes Odia support has been added to the best data anonymization library, Faker. $ pip install Faker >>> from faker import Faker >>> fake = Faker(\"or_IN\") >>> for _ in range(10): ... print(fake.name()) ... \u0b1a\u0b3f\u0b24\u0b30\u0b02\u0b1c\u0b28 \u0b28\u0b28\u0b4d\u0b26\u0b3f \u0b30\u0b3e\u0b1c, \u0b30\u0b2c\u0b3f\u0b28\u0b3e\u0b30\u0b3e\u0b5f\u0b23 \u0b15\u0b47\u0b26\u0b3e\u0b30\u0b28\u0b3e\u0b25 \u0b2c\u0b30\u0b4d\u0b2e\u0b3e \u0b05\u0b2e\u0b30\u0b28\u0b3e\u0b25 \u0b38\u0b47\u0b20\u0b40 \u0b38\u0b3e\u0b32\u0b41\u0b1c\u0b3e, \u0b15\u0b33\u0b4d\u0b2a\u0b24\u0b30\u0b41 \u0b26\u0b47\u0b2c\u0b30\u0b3e\u0b1c \u0b30\u0b3e\u0b27\u0b3e\u0b30\u0b3e\u0b23\u0b40 \u0b2a\u0b4b\u0b26\u0b4d\u0b26\u0b3e\u0b30 \u0b30\u0b3e\u0b27\u0b41 \u0b2e\u0b24\u0b32\u0b41\u0b2c \u0b36\u0b24\u0b2a\u0b25\u0b40 \u0b30\u0b28\u0b4d\u0b27\u0b3e\u0b30\u0b40, \u0b38\u0b41\u0b36\u0b3e\u0b28\u0b4d\u0b24 \u0b17\u0b48\u0b3e\u0b24\u0b2e \u0b13\u0b30\u0b3e\u0b2e","title":"Fake Odia name generation"},{"location":"contributions/#localization-projects","text":"Various localization projects to make websites available in Odia language","title":"Localization projects"},{"location":"contributions/#telegram","text":"","title":"Telegram"},{"location":"contributions/#mozilla-firefox-in-progress","text":"","title":"Mozilla Firefox (In-Progress)"},{"location":"contributions/#duckduckgo","text":"","title":"Duckduckgo"},{"location":"contributions/#covid-19-website-unofficial","text":"","title":"COVID-19 website (Unofficial)"},{"location":"contributors/","text":"Major contributors to the project: Anjan Kumar Panda Dr. Arun Malik Krishna Kabi Soumendra Kumar Sahoo Subhadarshi Panda Subhashish Panigrahi","title":"Contributors"},{"location":"possible_projects/","text":"Home page | MT_Flow_Pipleine | Wishlist Possible projects # This list contains the proposals for the projects which can be started with Odia language. This list has been created keeping students, research scholars and hobbist in mind, who by little knowledge on this domain can learn and able to execute this project. Monolingual corpus Language detector Word tokenizer Word2vec preparation Word similarity Sentence tokenizer Random sentence generator Sentence similarity Stemming Synonym Part of Speech tagging Dependency parse tree creation Lemmatization Sentiment analysis Text classification Co-reference resolution Word(s) based sentence creation Random quotation generator Author specific Quotation generation Syntactically sentence correction Correct punctuation marks Autocomplete Spell corrector Sentence completion Automatic speech recognition (major project) Monolingual corpus # For many NLP related tasks a large monolingual corpus is a necessity. As the name suggest a monolingual corpus (MC) stores text of only one language. In this case it will be of Odia. The MC mainly consists of full sentences. Odia Wikipedia as an MC provider # The major provider of MC data can be found in Odia Wikipedia. The source code to crawl and extract Wikipedia data can be found here You can directly get the dump from the Wikimedia site itself, you can get data dump of both Wikisource (Odia books) and Wikipedia (Odia articles) from this site . Need to cleanup a bit as well. How-to do? # This is a data collection project. 1. Extract the content from various websites with CC license by writing web crawlers or taking wikipedia dump 2. Clear the corpus. 3. Prepare an Odia language sentences corpus. Language detector # Given a text string, detect its language. It should identify if Odia language text are given. Existing Algorithms # Google language detector Part of speech tagging # Stemming # Lemmatization # This Website's documentation work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License .","title":"Wishlist"},{"location":"possible_projects/#possible-projects","text":"This list contains the proposals for the projects which can be started with Odia language. This list has been created keeping students, research scholars and hobbist in mind, who by little knowledge on this domain can learn and able to execute this project. Monolingual corpus Language detector Word tokenizer Word2vec preparation Word similarity Sentence tokenizer Random sentence generator Sentence similarity Stemming Synonym Part of Speech tagging Dependency parse tree creation Lemmatization Sentiment analysis Text classification Co-reference resolution Word(s) based sentence creation Random quotation generator Author specific Quotation generation Syntactically sentence correction Correct punctuation marks Autocomplete Spell corrector Sentence completion Automatic speech recognition (major project)","title":"Possible projects"},{"location":"possible_projects/#monolingual-corpus","text":"For many NLP related tasks a large monolingual corpus is a necessity. As the name suggest a monolingual corpus (MC) stores text of only one language. In this case it will be of Odia. The MC mainly consists of full sentences.","title":" Monolingual corpus"},{"location":"possible_projects/#odia-wikipedia-as-an-mc-provider","text":"The major provider of MC data can be found in Odia Wikipedia. The source code to crawl and extract Wikipedia data can be found here You can directly get the dump from the Wikimedia site itself, you can get data dump of both Wikisource (Odia books) and Wikipedia (Odia articles) from this site . Need to cleanup a bit as well.","title":"Odia Wikipedia as an MC provider"},{"location":"possible_projects/#how-to-do","text":"This is a data collection project. 1. Extract the content from various websites with CC license by writing web crawlers or taking wikipedia dump 2. Clear the corpus. 3. Prepare an Odia language sentences corpus.","title":"How-to do?"},{"location":"possible_projects/#language-detector","text":"Given a text string, detect its language. It should identify if Odia language text are given.","title":" Language detector"},{"location":"possible_projects/#existing-algorithms","text":"Google language detector","title":"Existing Algorithms"},{"location":"possible_projects/#part-of-speech-tagging","text":"","title":" Part of speech tagging"},{"location":"possible_projects/#stemming","text":"","title":" Stemming"},{"location":"possible_projects/#lemmatization","text":"This Website's documentation work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License .","title":" Lemmatization"},{"location":"purpose/","text":"To develop an ecosystem for Odia language in text, audio and vision space, which will nurture and further help developments in these areas to interested individuals.","title":"Purpose"},{"location":"Machine_Translation/MT_Flow_Pipeline/","text":"Home page | MT_Flow_Pipleine | Wishlist Machine Translation Flow Pipeline # Update log # Date | Version | Author | Change details 22nd Sept'19 | 0.01 | Soumendra Kumar Sahoo | Initial draft created Objective # This document explains to various pipeline flows for training and querying of English to Odia the translation. I have tried to explain the concepts as low level as possible for an amateur technical person. Important Terms # Parallel pairs : English and Odia text pairs with same meaning. Corpus : A Data set with relevant text data Translation : Converting meaning of a text in one language to another language. Transliteration : Converting pronunciation of a text from one language to another. Machine Translation : Automatic Machine learning based translation Phrase : Part of a sentence without any particular meaning. Neural Machine Translation: Deep learning based automatic machine translation. Agent : Bot. Crawler : Move from one website to another website like a Spider. Domain : Business level like Healthcare, IT, Tourism, Religious text, etc. POS tags : Part of speech tags like Noun, Verb etc. NER : Named Entity Recognition like Person, Organization, Place, etc. Parse : Extracting information The entire pipeline can be divided into two parts: 1. Ingestion & 2. Querying Ingestion # Inserting parallel pairs to create the Machine Learning model. You can checkout the following diagram for the entire flow of the Ingestion pipeline. Data Collection # This part deals with collecting data from multiple sources. The data can be processed parallel pairs or in raw format. In a high level classification let us divide the process into three parts: Continuous data collectors or Active Agents & One time/Batch data collectors Community generated corpus 1. Active Agents/Bots # Agents means Bots, which will be running continuously 24x7 or a specific period of time to find out prospective parallel pairs. There can be many types of Agents: News website crawlers : Collecting and matching the headlines/content of the News websites across En and Or Localized website crawlers : Websites which are translated into English as well as Odia Social Media Tweets/Posts across Twitter/Facebook. : People posting Tweets/Posts in social media across multiple languages Process # It may consists of the following steps: 1. Get a list of prospective 1. News websites 2. Localized Websites & 3. Social media Accounts 2. Crawl through the content from present to past posts. 3. Detect Odia text We can write an Odia Matra detector 4. Detect the parallel English text. This can be achieved by: 1. Matching Dictionary based words 2. Followed by sentences 3. Transliteration can help here too to match the Pronouns. 2. Batch data collectors # The cases where we will get the data from a specific location that needs to be accessed for a short time period until we collect the entire data. This may be an one time activity or long source data refresh period. For example: Wikipedia Dumps : Wikipedia generates En-Or paragraph aligned dumps every week. Open sourced repositories/papers describing the location to collect the data Process # The detecting of the data set is completely manual process. There is no need of automate that also. May be for the Wikipedia dump we can run an Agent in a weekly schedule to get only the incremental data. The flow will be: 1. manually detect the data set 2. Write a Data collector to get the data from the location or manually download the data set from that location. That's it. The Data collector work is done. 3. Community generated corpus # Other than the above two methods of collecting the data. We will need help from the community to manually prepare the data. We may need data in following categories: 1. Parallel pairs 2. Parallel pairs with POS tags & 3. Parallel pairs with Named Entity Recognition (NER) data 4. Parallel pairs with domain classification These are not distinct categories. That means Parallel pairs are mandatory to provide. The rest all POS, NER and Domain are optional. If community can provide pairs with all POS, NER and Domain, that data will be treated as pure Gold. The format to collect the POS, NER and Domain information yet to be decided. Data Preprocessing # The data we receive in the Collection phase are most likely not in the desired format and needs to be processed with additional metadata information to bring the collections into a standard format. These methods to convert raw data into standard format will happen at this stage. There are many crucial steps: 1. Data License verification 2. Data cleaning 1. HTML/CSS tags removal 2. Punctuation removal 3. English word removal from the Odia part of the pair 3. Alignment (Phrase, Sentence, Paragraph) 4. Metadata generation/addition 1. POS tags 2. NER 5. Filtering out the pairs based on a threshold. 6. Classification 1. Business domain based (IT, Religion, Tourism, Politics, etc.) 2. Morphology based (Word, Phrase, Sentence, Paragraph, etc.) 7. Converting the input raw format into standard format Let is go though these steps individually: 1. Data License verification # The data which we will receive through the Data collectors need to be checked. Provide proper Attributions wherever needed. Weed out the corpus where proper license is not there. Contact the original authors in case needed. Licensing is a crucial thing in Open source projects. Therefore, should not be taken lightly. Legal obligations may need to be faced if not handled smartly. 2. Data Cleaning # The data we received need to go through a set of cleaning steps before going further. 1. Remove duplicates from the corpus. 2. For the initial days, we may need to remove the punctuation marks from the pairs. later on during tune up we may need to enable the punctuation marks. 3. HTML and CSS tags are definitely need to be removed. A program can be written for this. 4. It may occur that there might be English words present in the Odia part of the pair. May be due to Brand name or Trademark. In these cases we may need to use 1. Keep it as it is (Recommendation) or 1. NER and POS will come into the picture here. 2. it will identify if the English word is a Pronoun. 3. If Yes, keep it, else try out other two options. 2. Transliteration to substitute the English word into Odia. 3. Remove that sentence 5. Again remove duplicates from the corpus. In fact we should remove duplicates after each step to reduce the workload on the next steps. 3. Data Alignment # Alignment of the pairs is the crucial part of this process. An entire codebase can be written to align the pairs. We have thousands of corpus lying around unusable due to this Alignment issue. The Alignment can be done based on the original text Morphology. 1. A dictionary based approach (matching words across the pairs) might be tried to align the sentences. However a strong plan needs to be created. 4. Metadata addition # Odia does not have any open source free auto POS tagger or NER taggers. There is word2vec present, which I need to test though. In combination of these features the translation accuracy will shoot up. For this I has asked these info from the community in the Data collection phase. 5. Threshold Filtering # After all these steps there will be a threshold set to analyze the validity and uniqueness of the pair. The threshold can be set based on: 1. Number of minimum letters need to be present in a sentence pair. 2. Number of minimum words need to be present in a sentence pair. 3. Percentage of English letters in an Odia part of the pair. 4. Minimum number of high weight POS tags like Nouns/Adjectives/Pronouns/Verbs needed to be declared as a valid pair. If any pair unable to pass any of the above conditions, should be filtered out. 6. Classification of the pairs # 1. Business domain based # We can not make a swiss army knife for any kind of translation done by a generic model. We have to find our niche domain. For the initial stage it may be the domain on which we get the maximum number of pairs. This concept is very critical and need to understood at early stage. You can not train with Agriculture data and want to test those with Medical terms. The result will be pathetic. Thats why those MT models who grow initially pick a specific sector and specialize on that first. If we need to specialize in generic Hi, Bye terms we need to weed out the other domain specific data pairs. Due to this reason the domain based classification is critical during the processing phase both for the MT model and business too. 2. Morphology based # We need to classify the pairs into words, phrases and sentences. Raw --> Standard # Convert the format to standard format and make any changes if needed to the data which will be used further on the training process. Query # Checking: - Check the minimum number of letters/words/POS tags - Likelihood of finding a translation - Depends on many parameters This Website's documentation work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License .","title":"Ingestion pipeline flow"},{"location":"Machine_Translation/MT_Flow_Pipeline/#machine-translation-flow-pipeline","text":"","title":"Machine Translation Flow Pipeline"},{"location":"Machine_Translation/MT_Flow_Pipeline/#update-log","text":"Date | Version | Author | Change details 22nd Sept'19 | 0.01 | Soumendra Kumar Sahoo | Initial draft created","title":"Update log"},{"location":"Machine_Translation/MT_Flow_Pipeline/#objective","text":"This document explains to various pipeline flows for training and querying of English to Odia the translation. I have tried to explain the concepts as low level as possible for an amateur technical person.","title":"Objective"},{"location":"Machine_Translation/MT_Flow_Pipeline/#important-terms","text":"Parallel pairs : English and Odia text pairs with same meaning. Corpus : A Data set with relevant text data Translation : Converting meaning of a text in one language to another language. Transliteration : Converting pronunciation of a text from one language to another. Machine Translation : Automatic Machine learning based translation Phrase : Part of a sentence without any particular meaning. Neural Machine Translation: Deep learning based automatic machine translation. Agent : Bot. Crawler : Move from one website to another website like a Spider. Domain : Business level like Healthcare, IT, Tourism, Religious text, etc. POS tags : Part of speech tags like Noun, Verb etc. NER : Named Entity Recognition like Person, Organization, Place, etc. Parse : Extracting information The entire pipeline can be divided into two parts: 1. Ingestion & 2. Querying","title":"Important Terms"},{"location":"Machine_Translation/MT_Flow_Pipeline/#ingestion","text":"Inserting parallel pairs to create the Machine Learning model. You can checkout the following diagram for the entire flow of the Ingestion pipeline.","title":"Ingestion"},{"location":"Machine_Translation/MT_Flow_Pipeline/#data-collection","text":"This part deals with collecting data from multiple sources. The data can be processed parallel pairs or in raw format. In a high level classification let us divide the process into three parts: Continuous data collectors or Active Agents & One time/Batch data collectors Community generated corpus","title":"Data Collection"},{"location":"Machine_Translation/MT_Flow_Pipeline/#1-active-agentsbots","text":"Agents means Bots, which will be running continuously 24x7 or a specific period of time to find out prospective parallel pairs. There can be many types of Agents: News website crawlers : Collecting and matching the headlines/content of the News websites across En and Or Localized website crawlers : Websites which are translated into English as well as Odia Social Media Tweets/Posts across Twitter/Facebook. : People posting Tweets/Posts in social media across multiple languages","title":"1. Active Agents/Bots"},{"location":"Machine_Translation/MT_Flow_Pipeline/#process","text":"It may consists of the following steps: 1. Get a list of prospective 1. News websites 2. Localized Websites & 3. Social media Accounts 2. Crawl through the content from present to past posts. 3. Detect Odia text We can write an Odia Matra detector 4. Detect the parallel English text. This can be achieved by: 1. Matching Dictionary based words 2. Followed by sentences 3. Transliteration can help here too to match the Pronouns.","title":"Process"},{"location":"Machine_Translation/MT_Flow_Pipeline/#2-batch-data-collectors","text":"The cases where we will get the data from a specific location that needs to be accessed for a short time period until we collect the entire data. This may be an one time activity or long source data refresh period. For example: Wikipedia Dumps : Wikipedia generates En-Or paragraph aligned dumps every week. Open sourced repositories/papers describing the location to collect the data","title":"2. Batch data collectors"},{"location":"Machine_Translation/MT_Flow_Pipeline/#process_1","text":"The detecting of the data set is completely manual process. There is no need of automate that also. May be for the Wikipedia dump we can run an Agent in a weekly schedule to get only the incremental data. The flow will be: 1. manually detect the data set 2. Write a Data collector to get the data from the location or manually download the data set from that location. That's it. The Data collector work is done.","title":"Process"},{"location":"Machine_Translation/MT_Flow_Pipeline/#3-community-generated-corpus","text":"Other than the above two methods of collecting the data. We will need help from the community to manually prepare the data. We may need data in following categories: 1. Parallel pairs 2. Parallel pairs with POS tags & 3. Parallel pairs with Named Entity Recognition (NER) data 4. Parallel pairs with domain classification These are not distinct categories. That means Parallel pairs are mandatory to provide. The rest all POS, NER and Domain are optional. If community can provide pairs with all POS, NER and Domain, that data will be treated as pure Gold. The format to collect the POS, NER and Domain information yet to be decided.","title":"3. Community generated corpus"},{"location":"Machine_Translation/MT_Flow_Pipeline/#data-preprocessing","text":"The data we receive in the Collection phase are most likely not in the desired format and needs to be processed with additional metadata information to bring the collections into a standard format. These methods to convert raw data into standard format will happen at this stage. There are many crucial steps: 1. Data License verification 2. Data cleaning 1. HTML/CSS tags removal 2. Punctuation removal 3. English word removal from the Odia part of the pair 3. Alignment (Phrase, Sentence, Paragraph) 4. Metadata generation/addition 1. POS tags 2. NER 5. Filtering out the pairs based on a threshold. 6. Classification 1. Business domain based (IT, Religion, Tourism, Politics, etc.) 2. Morphology based (Word, Phrase, Sentence, Paragraph, etc.) 7. Converting the input raw format into standard format Let is go though these steps individually:","title":"Data Preprocessing"},{"location":"Machine_Translation/MT_Flow_Pipeline/#1-data-license-verification","text":"The data which we will receive through the Data collectors need to be checked. Provide proper Attributions wherever needed. Weed out the corpus where proper license is not there. Contact the original authors in case needed. Licensing is a crucial thing in Open source projects. Therefore, should not be taken lightly. Legal obligations may need to be faced if not handled smartly.","title":"1. Data License verification"},{"location":"Machine_Translation/MT_Flow_Pipeline/#2-data-cleaning","text":"The data we received need to go through a set of cleaning steps before going further. 1. Remove duplicates from the corpus. 2. For the initial days, we may need to remove the punctuation marks from the pairs. later on during tune up we may need to enable the punctuation marks. 3. HTML and CSS tags are definitely need to be removed. A program can be written for this. 4. It may occur that there might be English words present in the Odia part of the pair. May be due to Brand name or Trademark. In these cases we may need to use 1. Keep it as it is (Recommendation) or 1. NER and POS will come into the picture here. 2. it will identify if the English word is a Pronoun. 3. If Yes, keep it, else try out other two options. 2. Transliteration to substitute the English word into Odia. 3. Remove that sentence 5. Again remove duplicates from the corpus. In fact we should remove duplicates after each step to reduce the workload on the next steps.","title":"2. Data Cleaning"},{"location":"Machine_Translation/MT_Flow_Pipeline/#3-data-alignment","text":"Alignment of the pairs is the crucial part of this process. An entire codebase can be written to align the pairs. We have thousands of corpus lying around unusable due to this Alignment issue. The Alignment can be done based on the original text Morphology. 1. A dictionary based approach (matching words across the pairs) might be tried to align the sentences. However a strong plan needs to be created.","title":"3. Data Alignment"},{"location":"Machine_Translation/MT_Flow_Pipeline/#4-metadata-addition","text":"Odia does not have any open source free auto POS tagger or NER taggers. There is word2vec present, which I need to test though. In combination of these features the translation accuracy will shoot up. For this I has asked these info from the community in the Data collection phase.","title":"4. Metadata addition"},{"location":"Machine_Translation/MT_Flow_Pipeline/#5-threshold-filtering","text":"After all these steps there will be a threshold set to analyze the validity and uniqueness of the pair. The threshold can be set based on: 1. Number of minimum letters need to be present in a sentence pair. 2. Number of minimum words need to be present in a sentence pair. 3. Percentage of English letters in an Odia part of the pair. 4. Minimum number of high weight POS tags like Nouns/Adjectives/Pronouns/Verbs needed to be declared as a valid pair. If any pair unable to pass any of the above conditions, should be filtered out.","title":"5. Threshold Filtering"},{"location":"Machine_Translation/MT_Flow_Pipeline/#6-classification-of-the-pairs","text":"","title":"6. Classification of the pairs"},{"location":"Machine_Translation/MT_Flow_Pipeline/#1-business-domain-based","text":"We can not make a swiss army knife for any kind of translation done by a generic model. We have to find our niche domain. For the initial stage it may be the domain on which we get the maximum number of pairs. This concept is very critical and need to understood at early stage. You can not train with Agriculture data and want to test those with Medical terms. The result will be pathetic. Thats why those MT models who grow initially pick a specific sector and specialize on that first. If we need to specialize in generic Hi, Bye terms we need to weed out the other domain specific data pairs. Due to this reason the domain based classification is critical during the processing phase both for the MT model and business too.","title":"1. Business domain based"},{"location":"Machine_Translation/MT_Flow_Pipeline/#2-morphology-based","text":"We need to classify the pairs into words, phrases and sentences.","title":"2. Morphology based"},{"location":"Machine_Translation/MT_Flow_Pipeline/#raw-standard","text":"Convert the format to standard format and make any changes if needed to the data which will be used further on the training process.","title":"Raw --&gt; Standard"},{"location":"Machine_Translation/MT_Flow_Pipeline/#query","text":"Checking: - Check the minimum number of letters/words/POS tags - Likelihood of finding a translation - Depends on many parameters This Website's documentation work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License .","title":"Query"},{"location":"Machine_Translation/progress/","text":"Home page | MT_Flow_Pipleine | Wishlist Machine translation from English to Odia language Analysis so far: # Machine Translation (MT) has started as early as on 1950s. Based on the progress on this field, the MT can be broadly categorized into following types: - Rule based MT (RBMT) - Statistical MT (SMT) and - Neural MT (NMT) - Hybrid MT (HMT) The NMT is giving best score (BLEU score) followed by SMT and RBMT. As explained in this paper for Indic languages SMT is performing better (at least 10% higher) than RBMT. Based on the reading and analysis from some existing papers, as the corpus is low, for the time-being we will go ahead with SMT ( Statistical Machine Translation ) first. As the corpus grows, we will start testing our luck in NMT ( Neural Machine Translation ) High level Roadmap # This road map is prepared based on my extra time and availability to work. If I will get more help we can deliver early. Month Year Milestone Status December 2018 Analyze and study the existing resources available on Internet Completed January 2019 Study the reference papers and experts in NMT and analyze their opinions Completed February 2019 Do same as January, concentrate more on the state-of-the-art practices Completed Mar-Dec 2019 Parallel corpora generation September 2019 Data ingestion pipeline Initial draft prepared October 2019 Read existing papers on MT and write the summary Delegated Nov-Dec 2019 Parallel corpora generation and data cleaning In-progress Further progress you can see over: https://github.com/OdiaNLP The parallel corpora generation data has been moved to Odia Wikimedia . There will be no further work unless we have achieved at least 10k (12k/10k achieved) parallel corpus. Detailed works completed in December 2018 # Found corpus worth around 27,000 English-Odia tab separated translation pairs This needs work on the followings: Some preprocessing need to be done. All translation not so accurate based on few manual review Spelling mistakes are there After reading one article on preprocessing English-Hindi corpus for SMT, I got insight on the following preprocessing tasks need to be done at first: Punctuation should NOT be removed English text should be lower cased as Odia does not have any upper case or lower case Spell normalization (Which is like Lemmatization ) will have a greater impact The impact of mapping numbers with unique class labels is not very effective and can be left out. We need NER (Named Entity Recognition) words for which Transliteration is enough We need POS tag data to improve the accuracy also. Moses MT tool is open source and ideal for developing SMT for our purpose. It needs parallel corpus to train and build the model after that prediction it can do. Data curation as explained on above point will be crucial to train this kind of system. If no better option available, we will mostly go with this approach. Other than this I got a good sense to keep an eye on Hybrid Machine Translation , which will be mixture of RBMT and SMT. There is not enough corpus available to go for NMT/SMT currently. We need to prepare a platform like Google translate community where we can manually create English-Odia phrase or word pairs. For this the following needed: UI hosted somewhere on cloud which can handle at least 100 requests per minute A DB, also hosted in cloud to store the data provided by users Possible suggestions for new users to start translating Review mechanism on the translated terms, even with good intention grammatical or syntactical errors need to be validated This infrastructure need to be hosted somewhere on cloud and it should be absolutely free to use. Unknown words handling : There will be definitely terms will come for which Odia words will not be present. These are some suggestions to handle unknown words: Transliterate those words to Odia Using some other existing translation system, convert those words to Hindi/Sanskrit then transliterate those words to Odia. Because most of the words in between Hindi/Sanskrit and Odia are same and people can understand. Detailed works completed in January 2019 # Last year has been pretty exciting to prepare this plan. Hope this year we will be able to deliver something useful for the community. Quality of data is highly needed. Therefore started preparing phrase parallel corpus and contributing simultaneously to both Google and Facebook. How ? Google translation community and Facebook Translate are recommending phrases to translate. The same phrases I am keeping a copy to myself. Microsoft has given an exciting resource to public to train their own Parallel Translation model that is also FREE . Details: It needs more than 10,000 pairs of parallel sentence pairs It seems to be working on SMT Train, Testing, Deployment all pipeline have been there First attempt I have tried this tool with GNOME translation pair I got. However, It got failed may be due to only 60 parallel sentences. In the meantime there seems to be many work going on with frequent papers in Unsupervised NMT for low resources languages, there has not been any significant usable work yet. However, I am keeping an eye on that too. The Translator Hub has been retired on May 2019. It has been migrated to Microsoft Custom translator . However, Odia language is unavailable currently. I have requested them to add it. Detailed works completed/ongoing in February 2019 # Data review and preparation started. Around 550 pairs reviewed till now. We need at least 10,000 pairs by end of this month. Finding ways to automate. Data Ingestion pipeline # A pipeline flow draft for data ingestion been created. Reading papers # Six Challenges for Neural Machine Translation # There are six challenges in NMT Domain mismatch Amount of Training Data Rare Words Long sentences Word Alignment Beam Search New pairs addition # Bible pairs from OdiEnCorp1.0 has been added to the consolidated corpus. NMT deployment # Microsoft Azure ML pricing (for basic version) It does not have GPU support Create and publish custom modules in Azure ML designer is not there Impediments # [x] Get at least 10,000 parallel open corpus for Odia language to begin with. [ ] Verification of the existing corpus badly needed. [ ] Moses does not run on Windows. Need an Ubuntu OS to test that. [ ] Need a cloud system to host manual translation API server and in future for online translation. Is it Microsoft ? Referred articles/websites: # Apertium Wiki for Odia language Indic Languages Multilingual Parallel Corpus The RGNLP Machine Translation Systems for WAT 2018 Anuvadaksh- An online existing English-Odia translator Wordnet for Odia RBMT vs SMT Detail MT system analysis of Indic languages English-Punjabi parallel corpus creation Creating more corpus by breaking long sentences Useful Open source libraries # fast_align : Align the words between two parallel corpus Data collected from: # Wikipedia Data dump Open Parallel Corpus OdiEnCorp 1.0 TDIL - Technical strings 52,000 pairs-Data needs to be cleaned Global Voices - 328 sentences pairs Mann ki baat - 1000+ pairs Twitter:DoctorBabu - Around 100 Botanical terms En-Or pairs Rupesh Ranjan Panda - Around 300 generic En-Or pairs Prospective data corpus # These are few places where relevant data may be present, however getting the data is not straight forward. * EMILLE Project : The Oriya written corpus consists of data incorporated from the CIIL Corpus, originally gathered by the Institute of Applied Language Sciences, Bhubaneshwar (approximately 2,730,000 words). * Gyan Nidhi-TDIL : Million pages\u2019 multilingual parallel text corpus in English and 11 Indian languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Marathi, Malayalam, Oriya, Punjabi, Tamil & Telugu) based on Unicode encoding. The Gyan Nidhi corpus contains the text in the form of books. In these books there were number of diagrams, figures, charts and other special symbols. These are removed from the text by using automated and manual tools. The text in gyan nidhi is in the form of paragraphs, that are converted into short sentences. * A Gold Standard Odia Raw Text Corpus : Around 15, 88, 287 words are there in this corpus available to purchase. \"In my dream of the 21st century for the State, I would have young men and women who put the interest of the State before them. They will have pride in themselves, confidence in themselves. They will not be at anybody\u2019s mercy, except their own selves. By their brains, intelligence and capacity, they will recapture the history of Kalinga.\" - Biju Pattnaik This Website's documentation work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License .","title":"Progress"},{"location":"Machine_Translation/progress/#analysis-so-far","text":"Machine Translation (MT) has started as early as on 1950s. Based on the progress on this field, the MT can be broadly categorized into following types: - Rule based MT (RBMT) - Statistical MT (SMT) and - Neural MT (NMT) - Hybrid MT (HMT) The NMT is giving best score (BLEU score) followed by SMT and RBMT. As explained in this paper for Indic languages SMT is performing better (at least 10% higher) than RBMT. Based on the reading and analysis from some existing papers, as the corpus is low, for the time-being we will go ahead with SMT ( Statistical Machine Translation ) first. As the corpus grows, we will start testing our luck in NMT ( Neural Machine Translation )","title":"Analysis so far:"},{"location":"Machine_Translation/progress/#high-level-roadmap","text":"This road map is prepared based on my extra time and availability to work. If I will get more help we can deliver early. Month Year Milestone Status December 2018 Analyze and study the existing resources available on Internet Completed January 2019 Study the reference papers and experts in NMT and analyze their opinions Completed February 2019 Do same as January, concentrate more on the state-of-the-art practices Completed Mar-Dec 2019 Parallel corpora generation September 2019 Data ingestion pipeline Initial draft prepared October 2019 Read existing papers on MT and write the summary Delegated Nov-Dec 2019 Parallel corpora generation and data cleaning In-progress Further progress you can see over: https://github.com/OdiaNLP The parallel corpora generation data has been moved to Odia Wikimedia . There will be no further work unless we have achieved at least 10k (12k/10k achieved) parallel corpus.","title":"High level Roadmap"},{"location":"Machine_Translation/progress/#detailed-works-completed-in-december-2018","text":"Found corpus worth around 27,000 English-Odia tab separated translation pairs This needs work on the followings: Some preprocessing need to be done. All translation not so accurate based on few manual review Spelling mistakes are there After reading one article on preprocessing English-Hindi corpus for SMT, I got insight on the following preprocessing tasks need to be done at first: Punctuation should NOT be removed English text should be lower cased as Odia does not have any upper case or lower case Spell normalization (Which is like Lemmatization ) will have a greater impact The impact of mapping numbers with unique class labels is not very effective and can be left out. We need NER (Named Entity Recognition) words for which Transliteration is enough We need POS tag data to improve the accuracy also. Moses MT tool is open source and ideal for developing SMT for our purpose. It needs parallel corpus to train and build the model after that prediction it can do. Data curation as explained on above point will be crucial to train this kind of system. If no better option available, we will mostly go with this approach. Other than this I got a good sense to keep an eye on Hybrid Machine Translation , which will be mixture of RBMT and SMT. There is not enough corpus available to go for NMT/SMT currently. We need to prepare a platform like Google translate community where we can manually create English-Odia phrase or word pairs. For this the following needed: UI hosted somewhere on cloud which can handle at least 100 requests per minute A DB, also hosted in cloud to store the data provided by users Possible suggestions for new users to start translating Review mechanism on the translated terms, even with good intention grammatical or syntactical errors need to be validated This infrastructure need to be hosted somewhere on cloud and it should be absolutely free to use. Unknown words handling : There will be definitely terms will come for which Odia words will not be present. These are some suggestions to handle unknown words: Transliterate those words to Odia Using some other existing translation system, convert those words to Hindi/Sanskrit then transliterate those words to Odia. Because most of the words in between Hindi/Sanskrit and Odia are same and people can understand.","title":" Detailed works completed in December 2018"},{"location":"Machine_Translation/progress/#detailed-works-completed-in-january-2019","text":"Last year has been pretty exciting to prepare this plan. Hope this year we will be able to deliver something useful for the community. Quality of data is highly needed. Therefore started preparing phrase parallel corpus and contributing simultaneously to both Google and Facebook. How ? Google translation community and Facebook Translate are recommending phrases to translate. The same phrases I am keeping a copy to myself. Microsoft has given an exciting resource to public to train their own Parallel Translation model that is also FREE . Details: It needs more than 10,000 pairs of parallel sentence pairs It seems to be working on SMT Train, Testing, Deployment all pipeline have been there First attempt I have tried this tool with GNOME translation pair I got. However, It got failed may be due to only 60 parallel sentences. In the meantime there seems to be many work going on with frequent papers in Unsupervised NMT for low resources languages, there has not been any significant usable work yet. However, I am keeping an eye on that too. The Translator Hub has been retired on May 2019. It has been migrated to Microsoft Custom translator . However, Odia language is unavailable currently. I have requested them to add it.","title":" Detailed works completed in January 2019"},{"location":"Machine_Translation/progress/#detailed-works-completedongoing-in-february-2019","text":"Data review and preparation started. Around 550 pairs reviewed till now. We need at least 10,000 pairs by end of this month. Finding ways to automate.","title":" Detailed works completed/ongoing in February 2019"},{"location":"Machine_Translation/progress/#data-ingestion-pipeline","text":"A pipeline flow draft for data ingestion been created.","title":" Data Ingestion pipeline"},{"location":"Machine_Translation/progress/#reading-papers","text":"","title":" Reading papers"},{"location":"Machine_Translation/progress/#six-challenges-for-neural-machine-translation","text":"There are six challenges in NMT Domain mismatch Amount of Training Data Rare Words Long sentences Word Alignment Beam Search","title":"Six Challenges for Neural Machine Translation"},{"location":"Machine_Translation/progress/#new-pairs-addition","text":"Bible pairs from OdiEnCorp1.0 has been added to the consolidated corpus.","title":" New pairs addition"},{"location":"Machine_Translation/progress/#nmt-deployment","text":"Microsoft Azure ML pricing (for basic version) It does not have GPU support Create and publish custom modules in Azure ML designer is not there","title":" NMT deployment"},{"location":"Machine_Translation/progress/#impediments","text":"[x] Get at least 10,000 parallel open corpus for Odia language to begin with. [ ] Verification of the existing corpus badly needed. [ ] Moses does not run on Windows. Need an Ubuntu OS to test that. [ ] Need a cloud system to host manual translation API server and in future for online translation. Is it Microsoft ?","title":"Impediments"},{"location":"Machine_Translation/progress/#referred-articleswebsites","text":"Apertium Wiki for Odia language Indic Languages Multilingual Parallel Corpus The RGNLP Machine Translation Systems for WAT 2018 Anuvadaksh- An online existing English-Odia translator Wordnet for Odia RBMT vs SMT Detail MT system analysis of Indic languages English-Punjabi parallel corpus creation Creating more corpus by breaking long sentences","title":"Referred articles/websites:"},{"location":"Machine_Translation/progress/#useful-open-source-libraries","text":"fast_align : Align the words between two parallel corpus","title":"Useful Open source libraries"},{"location":"Machine_Translation/progress/#data-collected-from","text":"Wikipedia Data dump Open Parallel Corpus OdiEnCorp 1.0 TDIL - Technical strings 52,000 pairs-Data needs to be cleaned Global Voices - 328 sentences pairs Mann ki baat - 1000+ pairs Twitter:DoctorBabu - Around 100 Botanical terms En-Or pairs Rupesh Ranjan Panda - Around 300 generic En-Or pairs","title":"Data collected from:"},{"location":"Machine_Translation/progress/#prospective-data-corpus","text":"These are few places where relevant data may be present, however getting the data is not straight forward. * EMILLE Project : The Oriya written corpus consists of data incorporated from the CIIL Corpus, originally gathered by the Institute of Applied Language Sciences, Bhubaneshwar (approximately 2,730,000 words). * Gyan Nidhi-TDIL : Million pages\u2019 multilingual parallel text corpus in English and 11 Indian languages (Assamese, Bengali, Gujarati, Hindi, Kannada, Marathi, Malayalam, Oriya, Punjabi, Tamil & Telugu) based on Unicode encoding. The Gyan Nidhi corpus contains the text in the form of books. In these books there were number of diagrams, figures, charts and other special symbols. These are removed from the text by using automated and manual tools. The text in gyan nidhi is in the form of paragraphs, that are converted into short sentences. * A Gold Standard Odia Raw Text Corpus : Around 15, 88, 287 words are there in this corpus available to purchase. \"In my dream of the 21st century for the State, I would have young men and women who put the interest of the State before them. They will have pride in themselves, confidence in themselves. They will not be at anybody\u2019s mercy, except their own selves. By their brains, intelligence and capacity, they will recapture the history of Kalinga.\" - Biju Pattnaik This Website's documentation work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License .","title":"Prospective data corpus"},{"location":"Machine_Translation/sentence_alignment/","text":"The biggest Issues I am receiving to collect the parallel corpus is sentences alignment. Issues: # The sentences are in an unordered format. Where the first sentence from one language might match with third sentence with second language. second one Possible Solutions: # Using a set of pre-translated words pairs to match among the sentence pairs. Using numbers among the pairs to match. Crawl the entire purnachandra bhashakosh and prepare the dictionary.","title":"Sentence Alignment"},{"location":"Machine_Translation/sentence_alignment/#issues","text":"The sentences are in an unordered format. Where the first sentence from one language might match with third sentence with second language. second one","title":"Issues:"},{"location":"Machine_Translation/sentence_alignment/#possible-solutions","text":"Using a set of pre-translated words pairs to match among the sentence pairs. Using numbers among the pairs to match. Crawl the entire purnachandra bhashakosh and prepare the dictionary.","title":"Possible Solutions:"}]}