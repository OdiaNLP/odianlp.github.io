{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>There has been less open source content on internet for low resource languages like Odia. If there also, all are present in a scattered way. This is an effort by few volunteers to increase the footprint over the Internet.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>To develop an ecosystem for Odia language in text, audio and vision space, which will nurture and further help developments in these areas to interested individuals.</p> <p>This project has been moved from MTEnglish2Odia.</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":"How can I start contributing to open source projects? <p>You can find a list of resources by <code>freecodecamp</code> on how to contribute. Do not get overwhelmed by this list, you can start with first two articles. Rest all you can read the title and go through based on your likability.</p> <ul> <li>Contributing to Open Source in general</li> <li>Direct GitHub searches</li> <li>Mozilla's contributor ecosystem</li> <li>Useful articles for new Open Source contributors</li> <li>Using Version Control</li> <li>Open Source books</li> <li>Open Source contribution initiatives</li> <li>Open Source programs to participate in</li> <li>License</li> </ul> How can I contribute to the projects under <code>OdiaNLP</code>? <ul> <li>OdiaNLP has few projects going on currently. Most of the projects are in data collection and data cleaning phase.</li> <li>We also want to start new projects. The possible projects lists are there and need helping hands to start making active contributions.</li> <li>You can send an email to get help from like minded people.</li> </ul>"},{"location":"#contributing-to-open-source-in-general","title":"Contributing to Open Source in general","text":"<ul> <li>The Definitive Guide to Contributing to Open Source by @DoomHammerNG</li> <li>An Intro to Open Source - Tutorials by DigitalOcean to guide you on your way to contribution success here on GitHub.</li> <li>Issuehub.pro - a tool for searching GitHub issues by label and language.</li> <li>Code Triage - another, really nice, tool for finding popular repositories and issues filtered by language.</li> <li>Awesome-for-beginners - a GitHub repo that amasses projects with good bugs for new contributors, and applies labels to describe them.</li> <li>Open Source Guides - Collection of resources for individuals, communities, and companies who want to learn how to run and contribute to an Open Source project.</li> <li>45 Github Issues Dos and Don\u2019ts - Do's and Don'ts on GitHub.</li> <li>GitHub Guides - basic guides on how to use GitHub effectively.</li> <li>Contribute to Open Source - Learn the GitHub workflow by contributing code to a simulation project.</li> <li>Linux Foundation's Open Source Guides for the Enterprise - The Linux Foundation's guides to Open Source projects.</li> <li>CSS Tricks An Open Source Etiquette Guidebook - An Open Source Etiquette Guidebook, written by Kent C. Dodds And Sarah Drasner.</li> <li>A to Z Resources for Students - Curated list of resources and opportunities for college students to learn a new coding language.</li> <li>Pull Request Roulette - This site has a list of pull requests submitted for review belonging to Open Source projects hosted on Github.</li> <li>\"How to Contribute to an Open Source Project on GitHub\" by Egghead.io - A step-by-step video guide of how to start contributing to Open Source projects on GitHub.</li> <li>Contributing to Open Source: A Live Walkthrough from Beginning to End - This walkthrough of an open source contribution covers everything from picking a suitable project, working on an issue, to getting the PR merged in.</li> <li>\"How to Contribute to Open Source Project by\" Sarah Drasner - They are focusing on the nitty-gritty of contributing a pull request (PR) to someone else\u2019s project on GitHub.</li> <li>\"How to get started with Open Source by\" Sayan Chowdhury - This article covers the resources for contributing to open source for beginners based on their favorite language interest.</li> <li>\"Browse good first issues to start contributing to open source\" - GitHub now helps you find good first issues to start contributing to open source.</li> <li>\"How to Contribute to Open Source Project\" by Maryna Z - This comprehensive article is directed towards businesses (but still useful for individual contributors) where it talks about why, how, and what open-source projects to contribute to.</li> <li>\"start-here-guidelines\" by Andrei - Lets Git started in the world of opensource, starting in the opensource playground. Especially designed for education and practical experience purposes.</li> <li>\"Getting Started with Open Source\" by NumFocus - a GitHub repo that helps contributors overcome barriers to entry in open-source.</li> <li>\"Opensoure-4-everyone\" by Chryz-hub  - A repository on everything related to open source. This is a project to help with GitHub membership visibility, practice with basic and advance git commands, getting started with open source, and more.</li> <li>\"Open Advice\" - Knowledge collection from a wide variety of Free Software projects. It answers the question what 42 prominent contributors would have liked to know when they started so you can get a head-start no matter how and where you contribute.</li> <li>\"GitHub Learning Lab\" - Level up your skills with GitHub Learning Lab. Our friendly bot will take you through a series of fun, practical projects to learn the skills you need in no time\u2014and share helpful feedback along the way.</li> <li>\"Ten simple rules for helping newcomers become contributors to open projects\" - This article covers rules based on studies of many communities and experiences of members, leaders, and observers.</li> </ul>"},{"location":"#direct-github-searches","title":"Direct GitHub searches","text":"<p>Search links that point directly to suitable issues to contribute to on GitHub. - is:issue is:open label:beginner - is:issue is:open label:easy - is:issue is:open label:first-timers-only - is:issue is:open label:good-first-bug - is:issue is:open label:\"good first issue\" - is:issue is:open label:starter - is:issue is:open label:up-for-grabs</p>"},{"location":"#mozillas-contributor-ecosystem","title":"Mozilla's contributor ecosystem","text":"<ul> <li>Good First Bugs - bugs that developers have identified as a good introduction to the project.</li> <li>MDN Web Docs - help the MDN Web Docs team in documenting the web platform by fixing content issues and platform bugs.</li> <li>Mentored Bugs - bugs that have a mentor assigned who will be there on IRC to help you when you get stuck while working on a fix.</li> <li>Bugs Ahoy - a site dedicated to finding bugs on Bugzilla.</li> <li>Firefox DevTools - a site dedicated to bugs filed for the developer tools in the Firefox browser.</li> <li>What Can I Do For Mozilla\u200a-\u200afigure out what you can work on by answering a bunch of questions about your skill set and interests.</li> <li>Start Mozilla - a Twitter account that tweets about issues fit for contributors new to the Mozilla ecosystem.</li> </ul>"},{"location":"#useful-articles-for-new-open-source-contributors","title":"Useful articles for new Open Source contributors","text":"<ul> <li>How to choose (and contribute to) your first Open Source project by @GitHub</li> <li>How to find your first Open Source bug to fix by @Shubheksha</li> <li>First Timers Only by @kentcdodds</li> <li>Bring Kindness Back to Open Source by @shanselman</li> <li>Getting into Open Source for the First Time by @mcdonnelldean</li> <li>How to Contribute to Open Source by @GitHub</li> <li>How to Find a Bug in Your Code by @dougbradbury</li> <li>Mastering Markdown by @GitHub</li> <li>First mission: Contributors page by @forCrowd</li> <li>How to make your first Open Source contribution in just 5 minutes by @roshanjossey</li> <li>Hacktoberfest 2019: How you can get your free shirt\u200a\u2014\u200aeven if you\u2019re new to coding by @quincylarson</li> <li>A Bitter Guide To Open Source by @ken_wheeler</li> <li>A junior developer\u2019s step-by-step guide to contributing to Open Source for the first time by @LetaKeane</li> <li>Learn Git and GitHub Step By Step (on Windows) by @ows-ali</li> <li>Why Open Source and How? by @james-gallagher</li> <li>How to get started with Open Source - By Sayan Chowdhury</li> <li>What open-source should I contribute to by @kentcdodds</li> <li>An immersive introductory guide to Open-source by Franklin Okolie</li> <li>Getting started with contributing to open source by Zara Cooper</li> <li>Beginner's guide to open-source contribution by Sudipto Ghosh</li> <li>8 non-code ways to contribute to open source by OpenSource</li> </ul>"},{"location":"#using-version-control","title":"Using Version Control","text":"<ul> <li>Think Like (a) Git - Git introduction for \"advanced beginners,\" but are still struggling, in order to give you a simple strategy to safely experiment with git.</li> <li>Try Git - Learn Git in 15 minutes from within your browser for free.</li> <li>Everyday Git - A useful minimum set of commands for Everyday Git.</li> <li>Oh shit, git! - how to get out of common <code>git</code> mistakes described in plain English; also see Dangit, git! for the page without swears.</li> <li>Atlassian Git Tutorials - various tutorials on using <code>git</code>.</li> <li>GitHub Git Cheat Sheet (PDF)</li> <li>freeCodeCamp's Wiki on Git Resources</li> <li>GitHub Flow (42:06) - GitHub talk on how to make a pull request.</li> <li>GitHub Learning Resources - Git and GitHub learning resources.</li> <li>Pro Git - The entire Pro Git book, written by Scott Chacon and Ben Straub and published by Apress.</li> <li>Git-it - Step by step Git tutorial desktop app.</li> <li>Flight Rules for Git - A guide about what to do when things go wrong.</li> <li>Git Guide for Beginners in Spanish - A complete guide of slides about git and GitHub explained in Spanish. Una gu\u00eda completa de diapositivas sobre git y GitHub explicadas en Espa\u00f1ol.</li> <li>Git Kraken - Visual, cross-platform, and interactive <code>git</code> desktop application for version control.</li> <li>Git Tips - Collection of most commonly used git tips and tricks.</li> <li>Git Best Practices - Commit Often, Perfect Later, Publish Once: Git Best Practices.</li> <li>Git Interactive Tutorial - Learn Git in the most visual and interactive way.</li> <li>Complete Git and GitHub Tutorial (1:12:39) - Full Git and GitHub walkthrough by Kunal Kushwaha.</li> </ul>"},{"location":"#open-source-books","title":"Open Source books","text":"<ul> <li>Producing Open Source Software - Producing Open Source Software is a book about the human side of Open Source development. It describes how successful projects operate, the expectations of users and developers, and the culture of free software.</li> <li>Open Source Book Series - Learn more about Open Source and the growing Open Source movement with a comprehensive list of free eBooks from https://opensource.com.</li> <li>Software Release Practice HOWTO - This HOWTO describes good release practices for Linux and other Open-Source projects. By following these practices, you will make it as easy as possible for users to build your code and use it, and for other developers to understand your code and cooperate with you to improve it.</li> <li>Open Sources 2.0 : The Continuing Evolution (2005) - Open Sources 2.0 is a collection of insightful and thought-provoking essays from today's technology leaders that continues painting the evolutionary picture that developed in the 1999 book, Open Sources: Voices from the Revolution.</li> <li>The Architecture of Open Source Applications - Show how various aspects of Git work under the covers to enable distributed workflows, and how it differs from other version control systems (VCSs).</li> <li>Open Sources: Voices from the Open Source Revolution - Essays from open-source pioneers such as Linus Torvalds (Linux), Larry Wall (Perl), and Richard Stallman (GNU).</li> </ul>"},{"location":"#open-source-contribution-initiatives","title":"Open Source contribution initiatives","text":"<ul> <li>Up For Grabs - Contains projects with beginner-friendly issues</li> <li>First Timers Only - A list of bugs that are labelled \"first-timers-only\".</li> <li>First Contributions - Make your first Open Source contribution in 5 minutes. A tool and tutorial to help beginners get started with contributions. Here is the GitHub source code for the site and opportunity to make a contribution to the repository itself.</li> <li>Hacktoberfest - A program to encourage Open Source contributions. Earn gifts like t-shirts and stickers by making at least 4 pull requests in the month of October.</li> <li>24 Pull Requests - 24 Pull Requests is a project to promote Open Source collaboration during the month of December.</li> <li>Ovio - A platform with a curated selection of contributor-friendly projects. It has a powerful issue search tool and let's you save projects and issues for later.</li> <li>Google Summer of Code - An annually run paid program by Google focused on bringing more student developers into open-source software development.</li> <li>Rails Girls Summer of Code - A global fellowship program for women and non-binary coders where they work on existing open-source projects and expand their skillset.</li> <li>Major League Hacking Fellowship - A remote internship alternative for aspiring technologists where they build, or contribute to open-source projects.</li> </ul>"},{"location":"#open-source-programs-to-participate-in","title":"Open Source programs to participate in","text":"<ul> <li>Google Summer of Code</li> <li>FossAsia</li> <li>MLH Fellowship</li> <li>Outreachy</li> <li>Hacktoberfest</li> <li>CNCF</li> <li>Microsoft Reinforcement learning</li> </ul>"},{"location":"contributions/","title":"Contributions","text":"<p>All the contribution are Open source and freely available (with proper attribution) to the society. OdiaNLP has done either entire or partial contributions to the following projects:</p>"},{"location":"contributions/#text-to-speech-or-speech-to-text","title":"Text to Speech or Speech to Text","text":""},{"location":"contributions/#mozilla-common-voice","title":"Mozilla Common Voice","text":"<ul> <li>Speech corpora creation through Mozilla Common Voice.</li> <li>201MB Speech data has been prepared with purely volunteering efforts as of 21 July 2021.</li> </ul> <ul> <li>After downloading you will get a folder structure like this:</li> </ul> <pre><code>cv-corpus-&lt;version&gt;-&lt;date&gt;\n\u2514\u2500\u2500 or\n \u00a0\u00a0 \u251c\u2500\u2500 reported.tsv\n \u00a0\u00a0 \u251c\u2500\u2500 dev.tsv\n \u00a0\u00a0 \u251c\u2500\u2500 other.tsv\n \u00a0\u00a0 \u251c\u2500\u2500 test.tsv\n \u00a0\u00a0 \u251c\u2500\u2500 train.tsv\n \u00a0\u00a0 \u251c\u2500\u2500 validated.tsv\n \u00a0\u00a0 \u251c\u2500\u2500 partials/template\n    \u2514\u2500\u2500 clips\n        \u251c\u2500\u2500 common_voice_or_&lt;count&gt;.mp3\n        \u251c\u2500\u2500 common_voice_or_&lt;count&gt;.mp3\n        \u251c\u2500\u2500 common_voice_or_&lt;count&gt;.mp3\n        \u251c\u2500\u2500 common_voice_or_&lt;count&gt;.mp3\n        .\n        .\n        .\n        \u2514\u2500\u2500 common_voice_or_&lt;count&gt;.mp3\n</code></pre> <ul> <li>The <code>.tsv</code> files contain the odia sentences in odia script.</li> <li>The <code>.mp3</code> files contain the corresponding pronunciation audio of the script.</li> </ul>"},{"location":"contributions/#machine-translation","title":"Machine Translation","text":""},{"location":"contributions/#google-translation-api-wrapper","title":"Google Translation API wrapper","text":"<ul> <li>Odia has been added into Unofficial Google Translation API wrapper.</li> </ul> <pre><code>$ pip install googletrans\n&gt;&gt;&gt; from googletrans import Translator\n&gt;&gt;&gt; translator = Translator()\n&gt;&gt;&gt; translator.translate(\"Hello Odia people\", dest=\"or\").text\n# '\u0b28\u0b2e\u0b38\u0b4d\u0b15\u0b3e\u0b30 \u0b13\u0b21\u0b3f\u0b06 \u0b32\u0b4b\u0b15\u0b2e\u0b3e\u0b28\u0b47 |'\n</code></pre>"},{"location":"contributions/#data-anonymization","title":"Data Anonymization","text":""},{"location":"contributions/#fake-odia-name-generation","title":"Fake Odia name generation","text":"<ul> <li>For fake name generation purposes Odia support has been added to the best data anonymization library, Faker.</li> </ul> <pre><code>$ pip install Faker\n&gt;&gt;&gt; from faker import Faker\n&gt;&gt;&gt; fake = Faker(\"or_IN\")\n&gt;&gt;&gt; for _ in range(10):\n...     print(fake.name())\n... \n\u0b1a\u0b3f\u0b24\u0b30\u0b02\u0b1c\u0b28 \u0b28\u0b28\u0b4d\u0b26\u0b3f\n\u0b30\u0b3e\u0b1c, \u0b30\u0b2c\u0b3f\u0b28\u0b3e\u0b30\u0b3e\u0b5f\u0b23\n\u0b15\u0b47\u0b26\u0b3e\u0b30\u0b28\u0b3e\u0b25 \u0b2c\u0b30\u0b4d\u0b2e\u0b3e\n\u0b05\u0b2e\u0b30\u0b28\u0b3e\u0b25 \u0b38\u0b47\u0b20\u0b40\n\u0b38\u0b3e\u0b32\u0b41\u0b1c\u0b3e, \u0b15\u0b33\u0b4d\u0b2a\u0b24\u0b30\u0b41\n\u0b26\u0b47\u0b2c\u0b30\u0b3e\u0b1c \u0b30\u0b3e\u0b27\u0b3e\u0b30\u0b3e\u0b23\u0b40 \u0b2a\u0b4b\u0b26\u0b4d\u0b26\u0b3e\u0b30\n\u0b30\u0b3e\u0b27\u0b41 \u0b2e\u0b24\u0b32\u0b41\u0b2c \u0b36\u0b24\u0b2a\u0b25\u0b40\n\u0b30\u0b28\u0b4d\u0b27\u0b3e\u0b30\u0b40, \u0b38\u0b41\u0b36\u0b3e\u0b28\u0b4d\u0b24\n\u0b17\u0b48\u0b3e\u0b24\u0b2e \u0b13\u0b30\u0b3e\u0b2e\n</code></pre>"},{"location":"contributions/#named-entity-recognition","title":"Named Entity Recognition","text":""},{"location":"contributions/#odia-persons-name-dataset","title":"Odia Persons' name dataset","text":"<ul> <li>Odia persons' name dataset has been added to Kaggle, to make it publicly available and further development on NER in Odia language.</li> </ul>"},{"location":"contributions/#localization","title":"Localization","text":"<ul> <li>Various localization projects to make websites and applications available in Odia language</li> </ul>"},{"location":"contributions/#telegram-open-source-instant-messaging-tool","title":"Telegram - Open source instant messaging tool","text":""},{"location":"contributions/#mozilla-firefox-in-progress","title":"Mozilla Firefox (In-Progress)","text":""},{"location":"contributions/#duckduckgo-privacy-based-search-engine","title":"Duckduckgo - Privacy based search engine","text":""},{"location":"contributions/#covid-19-website-unofficial","title":"COVID-19 website (Unofficial)","text":""},{"location":"contributions/#openodia-library","title":"OpenOdia Library","text":"<p>OpenOdia is a consolidated tool built for Odia language. It consists of various needed tools for Odia language like:</p> <ol> <li>Work tokenization</li> <li>Sentence tokenization</li> <li>Stopword removal</li> <li>Google Translate wrapper</li> <li>Automatic text summarization</li> <li>Odia name generation</li> </ol>"},{"location":"contributors/","title":"Contributors","text":"<p>Major contributors to the project:  </p> <ul> <li>Anjan Kumar Panda</li> <li>Dr. Arun Malik</li> <li>Krishna Kabi</li> <li>Kumarika</li> <li>Soumendra Kumar Sahoo</li> <li>Subhadarshi Panda</li> <li>Subhashish Panigrahi</li> </ul>"},{"location":"dictionary/","title":"Dictionary","text":"<p>This project has been started to prepare a English to Odia dictionary. However, to start with Odia to Odia dictionary has been given higher priority.  The following tasks are prepared as a roadmap.</p>"},{"location":"dictionary/#completed-work","title":"Completed work","text":"<ul> <li>Data scraping from Online Odia to Odia dictionary.<ul> <li>Odia to Odia dictionary dataset (138 MB) can be found at Kaggle dataset.</li> <li>This source code has been developed to fetch the data.</li> </ul> </li> </ul> Obsolete tasks"},{"location":"dictionary/#immediate-tasks","title":"Immediate Tasks","text":"<ul> <li> Cleaning of the dataset -&gt; Parked till further resource availability</li> <li> Extracting the missing pages dataset</li> </ul>"},{"location":"dictionary/#long-term-tasks","title":"Long term tasks","text":"<ul> <li>Hosting a dictionary.</li> <li>Using the dictionary for English to Odia translation.</li> <li>Exposing the dictionary as an API for the public.</li> </ul>"},{"location":"dictionary/#english-to-odia-dictionary","title":"English to Odia dictionary","text":"<p>To add on to the past activity of Odia to Odia dictionary, English to Odia dictionary task has been taken up.</p> Challenges <ol> <li>Initial English words corpus preparation.</li> <li>Finding Odia words of the corresponding English words.</li> <li>Cleaning up the pairs.</li> <li>What to do in case no matching English words found in the corpus?</li> </ol>"},{"location":"dictionary/#english-words-corpus","title":"English words corpus","text":"<p>To prepare the English words corpus, which will be used as the dataset the following minimum features were required.</p> Characteristics <ol> <li>The words list corpus should be available open source.</li> <li>As like most of the tasks in this project, there is a high chance that we will get parallel en-or word pairs. However, check if any parallel corpus found.</li> <li>It is better if we can find most used English words of generic domain. It's better to get domain specific words, but not in highest priority.</li> </ol> <ul> <li>With all these characteristics in mind, I have got an English wordlist from this GitHub repository.</li> <li>It was GPL 3.0 licensed and open sourced; good to start with.</li> <li>There are 2,33,464 words/phrases found in this word list.</li> </ul>"},{"location":"dictionary/#english-odia-word-pair","title":"English-Odia word pair","text":"<p>Now that we have got the list of English words. The next major challenge is to find the corresponding Odia words.</p> Resources available <p>We do have the following resources available:</p> <ol> <li>Get the parallel pairs from the existing parallel pairs available with us. These are manually curated by us.</li> <li>Take any of the 3rd party corpus, but again cleaning and all needs to be done. Nobody loves cleaning other people's garbage. Definitely not in this volunteering work.</li> <li>Use Bing or Google translate API to find those English words' Odia text.</li> </ol> <p>I have chosen to go ahead with the 3rd approach.</p> Challenges to use Google translate API <ol> <li>We need to use only free version of Google translate. I do not have money :D</li> <li>Google translate will rate limit the API calls for sure.12 Everybody does rate limiting. There are many advantages to it.3</li> </ol>"},{"location":"dictionary/#googletrans","title":"googletrans","text":"<ul> <li>Enter Googletrans python library, in which I am already a contributor.</li> <li>With <code>Googletrans</code>, I was able to translate from English to Odia words without any API token or credit card.</li> <li>For rate limiter constraints, I have thought to use multiple browser agents, asynchronous POST calls and random sleep time in between a batch of POST call requests.</li> <li>But, I got lazy and forgot these thoughts, simply used a <code>try</code> and <code>catch</code> block in synchronous POST calls, where after every rate limit exception the program sleeps for 2 minutes and tries again.</li> <li>This sleeping and calling continued till the end of all elements in the English word list.</li> <li>It has been 2 days 12+ hours and I have not reached even half of the words in the list.</li> <li>Now, I could not break away from the program otherwise duplicates may occur. Yeah, I know we can avoid that through program also, but you know I am lazy.</li> <li>Do not worry, about 1 lakh pairs I have got till now. I will upload these pairs to Kaggle dataset, so that you do not have to.</li> </ul>"},{"location":"dictionary/#cleaning-up-the-pairs","title":"Cleaning up the pairs","text":"<ul> <li>Those who have experience in Google translate, may know that Google translate results are good, but not fully accurate. Even I have contributed to Google translate (before Odia is available in their website) with level 17 contributor. Google translate needs help to translate accurately.</li> <li>There is a need for manual verification of the translated pairs. But, I am not sure when we will get time to do that.</li> <li>The other cleaning methods used:<ol> <li>All english words are lemmatized4 to their root form for e.g. <code>managers</code> becomes <code>manager</code>, <code>eaten</code> becomes <code>eat</code></li> <li>All words are converted to lower case.</li> <li>Spaces and newline characters are stripped off from the words.</li> </ol> </li> </ul>"},{"location":"dictionary/#in-case-an-exact-word-not-found","title":"In case an exact word not found","text":"<ul> <li>In case a word not found in the English word list as the exact match, check for the word in phrases i.e. multiple words, if it is found then respond back with that response.</li> </ul> Example <ul> <li>suppose <code>add</code> is not found in the English word list.</li> <li><code>add</code> will be searched in phrases</li> <li>if there are phrases like <code>add on</code>, <code>add up</code> those phrases will be picked and shown in the response.</li> </ul>"},{"location":"dictionary/#hosting-dictionary-api-on-server","title":"Hosting Dictionary API on server","text":"Challenges <ol> <li>The server needs to be in the free tier. I do not have money. :D</li> <li>Rate limiting number of requests from users.</li> <li>To make the API stable and scalable.</li> </ol> <ul> <li>I have started working in Heroku to deploy this dictionary.</li> <li>It's deployed in https://odia-dictionary.herokuapp.com/ but, it is not stable, due to 500 MB RAM free-tier limitation in Heroku.</li> </ul> Future Work <ul> <li> Host the dictionary in a stable server.</li> <li> Upload the English-Odia dictionary data to Kaggle.</li> </ul> <p>To cite this page, please use:</p> <pre><code>    @misc{OdiaNLP,\nauthor       = {Soumendra Kumar Sahoo},\ntitle        = {Dictionary by Odia NLP},\nhowpublished = {\\url{https://www.mte2o.com/}},\nyear         = {2021}\n}\n</code></pre> <ol> <li> <p>https://cloud.google.com/translate/quotas \u21a9</p> </li> <li> <p>https://stackoverflow.com/q/7646112/5014656 \u21a9</p> </li> <li> <p>https://nordicapis.com/everything-you-need-to-know-about-api-rate-limiting/ \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Lemmatisation \u21a9</p> </li> </ol>"},{"location":"possible_projects/","title":"Possible projects","text":"<p>This list contains the proposals for the projects which can be started with Odia language. This list has been created keeping students, research scholars and hobbist in mind, who by little knowledge on this domain can learn and able to execute this project.</p> <ul> <li>Monolingual corpus</li> <li>Language detector</li> <li>Word tokenizer</li> <li>Word2vec preparation</li> <li>Word similarity</li> <li>Sentence tokenizer</li> <li>Random sentence generator</li> <li>Sentence similarity</li> <li>Stemming</li> <li>Synonym</li> <li>Part of Speech tagging</li> <li>Dependency parse tree creation</li> <li>Lemmatization</li> <li>Sentiment analysis</li> <li>Text classification</li> <li>Co-reference resolution</li> <li>Word(s) based sentence creation</li> <li>Random quotation generator</li> <li>Author specific Quotation generation</li> <li>Syntactically sentence correction</li> <li>Correct punctuation marks</li> <li>Autocomplete</li> <li>Spell corrector</li> <li>Sentence completion</li> <li>Automatic speech recognition (major project)</li> <li>Text Summarization</li> <li>Optical Character Recognition</li> </ul>"},{"location":"possible_projects/#monolingual-corpus","title":"Monolingual corpus","text":"<ul> <li>Please check the resources page for existing corpus details.</li> </ul>"},{"location":"possible_projects/#language-detector","title":"Language detector","text":"<ul> <li>Given a text string, detect its language. It should identify if Odia language text are given.</li> <li>An existing language detector can be found in OpenOdia project.</li> </ul>"},{"location":"possible_projects/#existing-algorithms","title":"Existing Algorithms","text":"<ol> <li>Google language detector</li> </ol>"},{"location":"possible_projects/#part-of-speech-tagging","title":"Part of speech tagging","text":"<p>Given a sentence find out the part of speeches present on that sentence. Part of speech can be verb, noun, adjective, pronoun, preposition, etc.</p>"},{"location":"possible_projects/#stemming","title":"Stemming","text":"Initial rough corpus <ul> <li>[\"\u0b32\u0b47\", \"\u0b20\u0b41\", \"\u0b30\", \"\u0b30\u0b47\", \"\u0b1f\u0b3f\", \"\u0b1f\u0b47\", \"\u0b1f\u0b3e\",</li> <li>\u0b41\u0b25\u0b3f\u0b32\u0b47</li> <li>[\"\u0b25\u0b3f\u0b32\u0b47\", \"\u0b25\u0b3f\u0b32\", \"\u0b25\u0b3f\u0b32\u0b41\", \"\u0b25\u0b3f\u0b32\u0b3f\",      \"\u0b09\u0b1b\u0b47\", \"\u0b09\u0b1b\", \"\u0b09\u0b1b\u0b41\", \"\u0b09\u0b1b\u0b3f\",      \"\u0b07\u0b1b\u0b47\", \"\u0b07\u0b1b\", \"\u0b07\u0b1b\u0b41\", \"\u0b07\u0b1b\u0b3f\",      \"\u0b05\u0b1b\u0b47\", \"\u0b05\u0b1b\", \"\u0b05\u0b1b\u0b41\", \"\u0b05\u0b1b\u0b3f\",     \"\u0b38\u0b3f\u0b1b\u0b47\", \"\u0b38\u0b3f\u0b1b\", \"\u0b38\u0b3f\u0b1b\u0b41\", \"\u0b38\u0b3f\u0b1b\u0b3f\",     \"\u0b05\u0b28\u0b4d\u0b24\u0b47\", \"\u0b05\u0b28\u0b4d\u0b24\", \"\u0b05\u0b28\u0b4d\u0b24\u0b41\", \"\u0b05\u0b28\u0b4d\u0b24\u0b3f\",     \"\u0b07\u0b32\u0b47\", \"\u0b07\u0b32\", \"\u0b07\u0b32\u0b41\", \"\u0b07\u0b32\u0b3f\",     \"\u0b07\u0b2c\u0b47\", \"\u0b07\u0b2c\", \"\u0b07\u0b2c\u0b41\", \"\u0b07\u0b2c\u0b3f\",     \"\u0b25\u0b3f\u0b2c\u0b47\", \"\u0b25\u0b3f\u0b2c\", \"\u0b25\u0b3f\u0b2c\u0b41\", \"\u0b25\u0b3f\u0b2c\u0b3f\",     \"\u0b1f\u0b3e\u0b15\u0b41\", \"\u0b1f\u0b3e\u0b15\u0b47\", \"\u0b1f\u0b3f\u0b30\", \"\u0b1f\u0b3f\u0b30\u0b47\", \"\u0b1f\u0b3f\u0b0f\", \"\u0b2e\u0b3e\u0b28\u0b47\", \"\u0b17\u0b41\u0b21\u0b3c\u0b3e\"]</li> <li>[\"\u0b17\u0b41\u0b21\u0b3c\u0b3e\u0b0f\", \"\u0b17\u0b41\u0b21\u0b3c\u0b3e\u0b15\",] </li> </ul>"},{"location":"possible_projects/#largest-substring-approach","title":"Largest substring approach","text":"<ul> <li>By using the largest suffix substring removal process as shown in this code by Mohit for Hindi language. </li> <li>In Odia language by using a specific set of suffixes we can omit critical information form the sentence.</li> <li>For example the suffixes like uthilu, uthibe describes about the tense of the sentence, whether it is in future or past or present.</li> <li>Similarly, there will be exceptions throughout the process and we can not use a generic set of suffixes to stem.</li> <li>Therefore, a better method need to be found out.</li> </ul>"},{"location":"possible_projects/#existing-work","title":"Existing work","text":"<ul> <li>A Suffix Stripping Algorithm for Odia Stemmer by Utkal university with 88% accuracy.</li> <li>Design of Lightweight stemmer for Odia Derivational Suffixes by Govt. college of Engineering Keonjhar with 85% accuracy.</li> </ul>"},{"location":"possible_projects/#odia-text-summarization","title":"Odia text summarization","text":"<ul> <li>An existing extractive word-frequency based text summarizer is implemented in the OpenOdia project.  </li> <li>Extrative text summarization</li> <li>Automatic Text Summarization for Oriya Language</li> </ul>"},{"location":"possible_projects/#optical-character-recognition-of-odia-script","title":"Optical Character Recognition of Odia script","text":"<ul> <li>Automatic recognition of printed Oriya script</li> </ul>"},{"location":"projects/","title":"Projects","text":"<p>There are two major projects and many minor projects going on in parallel.  </p> <ul> <li>Machine Translation</li> <li>Dictionary &amp;</li> <li>Other minor projects</li> </ul>"},{"location":"wordEmbedding/","title":"Word Embeddings in Odia language [In-progress]","text":"<p>In this article we will go through the followings:</p> <ol> <li>Introduction to Word embeddings</li> <li>Design and Architecture</li> <li>Implementation for Odia language</li> <li>Usability</li> <li>Conclusion</li> <li>Future work</li> </ol>"},{"location":"wordEmbedding/#introduction","title":"Introduction","text":"<ul> <li>In text processing, one can split a bulk of paragraphs or pages into words (or tokens in NLP).</li> <li> <p>In NLP, words can be represented as dense vectors.</p> What is dense vector? <p>There are two kinds of vector representation in NLP for representation purpose.</p> <ol> <li>Sparse Vector<ul> <li>Sparse means in this there will be number of zeroes like [0,0,1,0,0,1,0]</li> </ul> </li> <li>Dense Vector<ul> <li>Dense vector represents the same data in non-zero values like [(2, 5), (1, 1)]</li> <li>This is an example, where it is a unique combination of the position and value of the elements in the matrix.</li> </ul> </li> </ol> </li> <li> <p>In <code>Word Embedding</code> the values are represented in such a way that the words which are similar to each other have similar vector representation.</p> </li> <li>If we take an example then <code>\u0b15\u0b32\u0b2e</code> (kalama) would have more similarity with <code>\u0b2a\u0b47\u0b28\u0b38\u0b3f\u0b32</code> (pencil) than with <code>\u0b38\u0b19\u0b4d\u0b17\u0b40\u0b24</code> (sangeeta).</li> <li>This similarity is represented by dense vectors.</li> </ul>"},{"location":"wordEmbedding/#design-and-architecture","title":"Design and Architecture","text":"<ul> <li> <p>There mainly three ways we can determine the word embeddings:  </p> <ol> <li>Word2Vec (Word to Vector)</li> <li>CBOW (Continuous Bag of Words) and </li> <li>GloVe (Global Vectors)</li> </ol> </li> <li> <p>Here we have implemented the word embedding using word2vec approach.</p> </li> </ul> <p>To cite this page, please use:</p> <pre><code>    @misc{OdiaNLP,\nauthor       = {Soumendra Kumar Sahoo},\ntitle        = {Word Embeddings by Odia NLP},\nhowpublished = {\\url{https://www.mte2o.com/}},\nyear         = {2021}\n}\n</code></pre>"},{"location":"NLP/course-toc/","title":"Table of Content","text":"<p>These are the topics which will be covered in this course.</p>"},{"location":"NLP/course-toc/#prerequisites","title":"Prerequisites","text":"<p>These are the basic knowledge needed to go ahead with this course.</p> <ol> <li>Basic Python programming knowledge which includes<ol> <li>Python data types: string, integer, list, set and dictionary</li> <li>Basic looping using for loop</li> <li>Conditional operators if..else</li> </ol> </li> </ol> But, I do not know Python programming  <p>Do not worry there are plenty of courses available for free online to learn Python. If you prefer text based tutorial or video based ones, all are available. You just need to search for it. Yeah, but you know something from me as a reference is not it? ok, these are my recommendations and yes of course all are free.</p>"},{"location":"NLP/course-toc/#text-based","title":"Text-based","text":"<ol> <li>30-days-of-python</li> <li>Automate the boring stuff</li> </ol>"},{"location":"NLP/course-toc/#video-based","title":"Video-based","text":"<ol> <li>Python for Beginners - Learn Python in 1 Hour</li> <li>Learn Python - Full Course for Beginners [Tutorial]</li> <li>Python Tutorial For Beginners In Hindi (With Notes) \ud83d\udd25</li> </ol>"},{"location":"NLP/course-toc/#introduction","title":"Introduction","text":"<ol> <li>Application of NLP in industry8 <ul> <li>There are many ways we can showcase how NLP is helping the world. We can start with a few examples:<ul> <li>Characters and the unicode behind them</li> <li>Summarization</li> <li>Sentiment analysis</li> </ul> </li> </ul> </li> <li>Curriculum</li> </ol>"},{"location":"NLP/course-toc/#python-and-nlp-basics","title":"Python and NLP basics","text":"<ol> <li>OS module</li> <li>Text and JSON files</li> <li>Docx</li> <li>PDF files</li> <li>NLTK Environment and Corpora</li> <li>Spacy</li> </ol>"},{"location":"NLP/course-toc/#nlp-operation","title":"NLP Operation","text":"<ol> <li>RegEx4</li> <li>Tokenization</li> <li>Bag of words model based on count and frequency</li> <li>Frequency Distribution</li> <li>Bigrams, Trigrams and Ngrams</li> <li>Stopwords</li> <li>Stemming</li> <li>Lemmatization 1</li> <li>POS tagging</li> <li>Named Entity Recognition</li> </ol>"},{"location":"NLP/course-toc/#sentence-structure-analysis","title":"Sentence Structure Analysis","text":"<ol> <li>Syntax Trees</li> <li>Chunking</li> <li>Chinking</li> <li>Automatic Text Paraphrasing</li> </ol>"},{"location":"NLP/course-toc/#word-vectors","title":"Word vectors","text":"<ol> <li>Word embedding2</li> <li>Word2vec</li> <li>GloVe6</li> <li>Topic Modeling</li> <li>Latent semantic analysis9</li> </ol>"},{"location":"NLP/course-toc/#text-classification","title":"Text Classification","text":"<ol> <li>Machine Learning Basics</li> <li>Bag of words model</li> <li>Count Vectorization</li> <li>Term Frequency</li> <li>Inverse Document Frequency</li> <li>Convert text to features and labels</li> <li>Multinomial Naive Bayes Classifier</li> <li>Confusion Matrix3</li> </ol>"},{"location":"NLP/course-toc/#deep-learning-with-nlp","title":"Deep Learning with NLP","text":"<ol> <li>Deep Learning Basics5</li> <li>RNN</li> <li>Attention7</li> <li>LSTM</li> <li>BeRT</li> <li>Transformers</li> </ol>"},{"location":"NLP/course-toc/#application","title":"Application","text":"<ol> <li>Text Mining and Information Extraction</li> <li>Sentiment Analysis</li> <li>Chat bots</li> <li>Text Summarization</li> <li>Machine Translation</li> <li>Spam detector</li> <li>Question Answering10</li> </ol> <ol> <li> <p>https://www.udemy.com/course/natural-language-processing-nlp-for-beginners-using-nltk-in-python/ \u21a9</p> </li> <li> <p>https://www.kaggle.com/learn/natural-language-processing \u21a9</p> </li> <li> <p>https://www.edureka.co/python-natural-language-processing-course \u21a9</p> </li> <li> <p>https://www.udemy.com/course/introduction-to-natural-language-processing/ \u21a9</p> </li> <li> <p>https://www.udemy.com/course/nlp-natural-language-processing-with-python/ \u21a9</p> </li> <li> <p>https://www.udemy.com/course/natural-language-processing-with-deep-learning-in-python/ \u21a9</p> </li> <li> <p>https://www.udacity.com/course/natural-language-processing-nanodegree--nd892 \u21a9</p> </li> <li> <p>https://data-flair.training/blogs/nlp-tutorial-natural-language-processing/ \u21a9</p> </li> <li> <p>https://www.udemy.com/course/data-science-natural-language-processing-in-python/ \u21a9</p> </li> <li> <p>https://www.udemy.com/course/nlp-with-transformers/ \u21a9</p> </li> </ol>"},{"location":"blogs/odias_in_ML/","title":"Learning from Odias in ML conference","text":""},{"location":"blogs/odias_in_ML/#speakers","title":"Speakers","text":"<ul> <li>Prof Panchanan Mohanty</li> <li>Prof Vivekananda Pani</li> </ul>"},{"location":"blogs/odias_in_ML/#key-points","title":"Key points","text":"<p>This is the consolidated note from the above two professors:</p> <ul> <li>There are two kinds of Odia language form: written and spoken Odia. The AI/ML process for Spoken Odia process has not started yet. Importance needs to be given on Written Odia.</li> <li>NLP for Odia is just getting started and there is a vast area of opportunity on every field of NLP in Odia.</li> <li>There are four major things needed in Odia for NLP:<ol> <li>Standard Font<ul> <li>Unicode font is not made consulting Odia linguists.</li> <li>The makers had taken shortcuts and not standardized the font</li> <li>ASCII creates better Odia</li> <li>Every algo will be suboptimal to English until this font issue is resolved.</li> <li>We need to learn this encodings</li> <li>ML has two aspects</li> <li>Data Normalization</li> <li>Data Cleaning</li> </ul> </li> <li>Rules for spelling<ul> <li>1990 1st Odia spell checker</li> <li>Still Odisha Govt could not integrate with further tools</li> <li>Spell checking need to be standardized.</li> </ul> </li> <li>Dictionary<ul> <li>No good dictionary after Purnachandra Bhashakosha</li> <li>Prepare dictionary using Lexicography</li> <li>Exhaustive dictionary need to be created.</li> <li>Get data from newspaper, publishers, prepare dictionary \u2192 Easy</li> </ul> </li> <li>Grammar<ul> <li>Descriptive standard grammar needs to be there.</li> <li>Book available on market are not Odia grammar</li> </ul> </li> </ol> </li> </ul>"},{"location":"machine_translation/MT_Flow_Pipeline/","title":"Machine Translation ingestion flow pipeline","text":""},{"location":"machine_translation/MT_Flow_Pipeline/#update-log","title":"Update log","text":"Date Year Version Author Change details 22nd Sept 2019 0.01 Soumendra Kumar Sahoo Initial draft created 7th Sept 2020 0.02 Soumendra Kumar Sahoo Update log converted to a tabular format"},{"location":"machine_translation/MT_Flow_Pipeline/#objective","title":"Objective","text":"<p>This document explains to various pipeline flows for training and querying of English to Odia the translation. I have tried to explain the concepts as low level as possible for an amateur technical person.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#important-terms","title":"Important Terms","text":"<ul> <li>Parallel pairs:  English and Odia text pairs with same meaning.</li> <li>Corpus: A Data set with relevant text data</li> <li>Translation: Converting meaning of a text in one language to another language.</li> <li>Transliteration: Converting pronunciation of a text from one language to another.</li> <li>Machine Translation: Automatic Machine learning based translation</li> <li>Phrase: Part of a sentence without any particular meaning.</li> <li>Neural Machine Translation: Deep learning based automatic machine translation.</li> <li>Agent: Bot.</li> <li>Crawler: Move from one website to another website like a Spider.</li> <li>Domain: Business level like Healthcare, IT, Tourism, Religious text, etc.</li> <li>POS tags: Part of speech tags like Noun, Verb etc.</li> <li>NER: Named Entity Recognition like Person, Organization, Place, etc.</li> <li>Parse: Extracting information</li> </ul> <p>The entire pipeline can be divided into two parts:  </p> <ol> <li>Ingestion &amp;</li> <li>Querying</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#ingestion","title":"Ingestion","text":"<p>Inserting parallel pairs to create the Machine Learning model. You can checkout the following diagram for the entire flow of the Ingestion pipeline.</p> <p> </p>"},{"location":"machine_translation/MT_Flow_Pipeline/#data-collection","title":"Data Collection","text":"<p>This part deals with collecting data from multiple sources. The data can be processed parallel pairs or in raw format. In a high level classification let us divide the process into three parts:</p> <ol> <li>Continuous data collectors or Active Agents &amp;</li> <li>One time/Batch data collectors</li> <li>Community generated corpus</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#1-active-agentsbots","title":"1. Active Agents/Bots","text":"<p>Agents means Bots, which will be running continuously 24x7 or a specific period of time to find out prospective parallel pairs. There can be many types of Agents:</p> <ol> <li>News website crawlers : Collecting and matching the headlines/content of the News websites across En and Or</li> <li>Localized website crawlers : Websites which are translated into English as well as Odia</li> <li>Social Media Tweets/Posts across Twitter/Facebook. : People posting Tweets/Posts in social media across multiple languages</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#process","title":"Process","text":"<p>It may consists of the following steps:  </p> <ol> <li>Get a list of prospective</li> <li>News websites</li> <li>Localized Websites &amp;</li> <li>Social media Accounts</li> <li>Crawl through the content from present to past posts.</li> <li>Detect Odia text We can write an Odia Matra detector</li> <li>Detect the parallel English text. This can be achieved by:</li> <li>Matching Dictionary based words</li> <li>Followed by sentences</li> <li>Transliteration can help here too to match the Pronouns.</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#2-batch-data-collectors","title":"2. Batch data collectors","text":"<p>The cases where we will get the data from a specific location that needs to be accessed for a short time period until we collect the entire data. This may be an one time activity or long source data refresh period. For example:</p> <ol> <li>Wikipedia Dumps  </li> <li>Wikipedia generates En-Or paragraph aligned dumps every week.*</li> <li>Open sourced repositories/papers describing the location to collect the data</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#process_1","title":"Process","text":"<p>The detecting of the data set is completely manual process. There is no need of automate that also. May be for the Wikipedia dump we can run an Agent in a weekly schedule to get only the incremental data. The flow will be:  </p> <ol> <li>manually detect the data set</li> <li>Write a Data collector to get the data from the location or manually download the data set from that location.</li> </ol> <p>That's it. The Data collector work is done.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#3-community-generated-corpus","title":"3. Community generated corpus","text":"<p>Other than the above two methods of collecting the data. We will need help from the community to manually prepare the data. We may need data in following categories:</p> <ol> <li>Parallel pairs</li> <li>Parallel pairs with POS tags &amp;</li> <li>Parallel pairs with Named Entity Recognition (NER) data</li> <li>Parallel pairs with domain classification</li> </ol> <p>These are not distinct categories. That means Parallel pairs are mandatory to provide. The rest all POS, NER and Domain are optional. If community can provide pairs with all POS, NER and Domain, that data will be treated as pure Gold. The format to collect the POS, NER and Domain information yet to be decided.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#data-preprocessing","title":"Data Preprocessing","text":"<p>The data we receive in the Collection phase are most likely not in the desired format and needs to be processed with additional metadata information to bring the collections into a standard format. These methods to convert raw data into standard format will happen at this stage.</p> <p>There are many crucial steps:  </p> <ol> <li>Data License verification</li> <li>Data cleaning</li> <li>HTML/CSS tags removal</li> <li>Punctuation removal</li> <li>English word removal from the Odia part of the pair</li> <li>Alignment (Phrase, Sentence, Paragraph)</li> <li>Metadata generation/addition</li> <li>POS tags</li> <li>NER</li> <li>Filtering out the pairs based on a threshold.</li> <li>Classification</li> <li>Business domain based (IT, Religion, Tourism, Politics, etc.)</li> <li>Morphology based (Word, Phrase, Sentence, Paragraph, etc.)</li> <li>Converting the input raw format into standard format</li> </ol> <p>Let is go though these steps individually:</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#1-data-license-verification","title":"1. Data License verification","text":"<ol> <li>The data which we will receive through the Data collectors need to be checked.</li> <li>Provide proper Attributions wherever needed.</li> <li>Weed out the corpus where proper license is not there.</li> <li>Contact the original authors in case needed.</li> </ol> <p>Licensing is a crucial thing in Open source projects. Therefore, should not be taken lightly. Legal obligations may need to be faced if not handled smartly.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#2-data-cleaning","title":"2. Data Cleaning","text":"<p>The data we received need to go through a set of cleaning steps before going further.  </p> <ol> <li>Remove duplicates from the corpus.</li> <li>For the initial days, we may need to remove the punctuation marks from the pairs. later on during tune up we may need to enable the punctuation marks.</li> <li>HTML and CSS tags are definitely need to be removed. A program can be written for this.</li> <li>It may occur that there might be English words present in the Odia part of the pair. May be due to Brand name or Trademark. In these cases we may need to use</li> <li>Keep it as it is (Recommendation) or<ol> <li>NER and POS will come into the picture here.</li> <li>it will identify if the English word is a Pronoun.</li> <li>If Yes, keep it, else try out other two options.</li> </ol> </li> <li>Transliteration to substitute the English word into Odia.</li> <li>Remove that sentence</li> <li>Again remove duplicates from the corpus. In fact we should remove duplicates after each step to reduce the workload on the next steps.</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#3-data-alignment","title":"3. Data Alignment","text":"<p>Alignment of the pairs is the crucial part of this process. An entire codebase can be written to align the pairs. We have thousands of corpus lying around unusable due to this Alignment issue. The Alignment can be done based on the original text Morphology.</p> <ol> <li>A dictionary based approach (matching words across the pairs) might be tried to align the sentences. However a strong plan needs to be created.</li> </ol>"},{"location":"machine_translation/MT_Flow_Pipeline/#4-metadata-addition","title":"4. Metadata addition","text":"<p>Odia does not have any open source free auto POS tagger or NER taggers. There is word2vec present, which I need to test though. In combination of these features the translation accuracy will shoot up. For this I has asked these info from the community in the Data collection phase.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#5-threshold-filtering","title":"5. Threshold Filtering","text":"<p>After all these steps there will be a threshold set to analyze the validity and uniqueness of the pair. The threshold can be set based on:  </p> <ol> <li>Number of minimum letters need to be present in a sentence pair.</li> <li>Number of minimum words need to be present in a sentence pair.</li> <li>Percentage of English letters in an Odia part of the pair.</li> <li>Minimum number of high weight POS tags like Nouns/Adjectives/Pronouns/Verbs needed to be declared as a valid pair.</li> </ol> <p>If any pair unable to pass any of the above conditions, should be filtered out.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#6-classification-of-the-pairs","title":"6. Classification of the pairs","text":""},{"location":"machine_translation/MT_Flow_Pipeline/#1-business-domain-based","title":"1. Business domain based","text":"<p>We can not make a swiss army knife for any kind of translation done by a generic model. We have to find our niche domain. For the initial stage it may be the domain on which we get the maximum number of pairs. This concept is very critical and need to understood at early stage. You can not train with Agriculture data and want to test those with Medical terms. The result will be pathetic. Thats why those MT models who grow initially pick a specific sector and specialize on that first. If we need to specialize in generic Hi, Bye terms we need to weed out the other domain specific data pairs. Due to this reason the domain based classification is critical during the processing phase both for the MT model and business too.  </p>"},{"location":"machine_translation/MT_Flow_Pipeline/#2-morphology-based","title":"2. Morphology based","text":"<p>We need to classify the pairs into words, phrases and sentences.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#raw-standard","title":"Raw \u2192 Standard","text":"<p>Convert the format to standard format and make any changes if needed to the data which will be used further on the training process.</p>"},{"location":"machine_translation/MT_Flow_Pipeline/#query","title":"Query","text":"<p>Checking:</p> <ul> <li>Check the minimum number of letters/words/POS tags</li> <li>Likelihood of finding a translation<ul> <li>Depends on many parameters</li> </ul> </li> </ul> <p>To cite this page, please use:</p> <pre><code>    @misc{OdiaNLP,\nauthor       = {Soumendra Kumar Sahoo},\ntitle        = {Ingestion pipeline for MT by Odia NLP},\nhowpublished = {\\url{https://www.mte2o.com/}},\nyear         = {2021}\n}\n</code></pre>"},{"location":"machine_translation/datasets/","title":"English-Odia Parallel corpus","text":""},{"location":"machine_translation/datasets/#overview","title":"Overview","text":"<ul> <li>80,437 English text followed by its Odia translation text pairs can be downloaded from our NMT model repo.</li> <li>Parallel pairs have been collected from many sources by many volunteers.</li> </ul>"},{"location":"machine_translation/datasets/#sources","title":"Sources","text":"<ul> <li>Wikipedia Data dump</li> <li>Open Parallel Corpus</li> <li>OdiEnCorp 1.0</li> <li>TDIL - Technical strings 52,000 pairs-Data needs to be cleaned</li> <li>Global Voices - 328 sentences pairs</li> <li>Mann ki baat - 1000+ pairs</li> <li>Twitter:DoctorBabu - Around 100 Botanical terms En-Or pairs</li> <li>Rupesh Ranjan Panda - Around 300 generic En-Or pairs</li> <li>Krishna Kabi - 186 En-Or pairs</li> </ul>"},{"location":"machine_translation/datasets/#additional-links","title":"Additional links","text":"<ul> <li>Extracting Parallel-text pairs from Wikipedia</li> </ul>"},{"location":"machine_translation/datasets/#odia-monolingual-corpus","title":"Odia Monolingual corpus","text":"<ul> <li>Monolingual Odia data has been extracted from Wikipedia.</li> <li>You can use this repo to fetch the latest dataset.</li> <li>Ready-made monolingual corpus (with ~17,000 wikipedia articles) can be found at Kaggle created by Gaurav.</li> </ul>"},{"location":"machine_translation/datasets/#odia-dictionary","title":"Odia dictionary","text":"<ul> <li>The dictionary data has been extracted from Odia Purnachandra Bhashakosha.</li> <li>The source code repository for the dataset are in: OdiaNLP/dictionary</li> </ul>"},{"location":"machine_translation/progress/","title":"Progress on Machine Translation","text":"<p>Machine translation from English to Odia language.</p>"},{"location":"machine_translation/progress/#analysis-so-far","title":"Analysis so far","text":"<p>Machine Translation (MT) has started as early as on 1950s. Based on the progress on this field, the MT can be broadly categorized into following types:</p> <ul> <li>Rule based MT (RBMT)</li> <li>Statistical MT (SMT) and</li> <li>Neural MT (NMT)</li> <li>Hybrid MT (HMT)</li> </ul> <p>The NMT is giving best score (BLEU score) followed by SMT and RBMT. As explained in this paper for Indic languages SMT is performing better (at least 10% higher) than RBMT. Based on the reading and analysis from some existing papers, as the corpus is low, for the time-being we will go ahead with SMT (Statistical Machine Translation) first. As the corpus grows, we will start testing our luck in NMT (Neural Machine Translation)</p>"},{"location":"machine_translation/progress/#high-level-roadmap","title":"High level Roadmap","text":"<p>This road map is prepared based on my extra time and availability to work. If I will get more help we can deliver early.</p> Month Year Milestone Status December 2018 Analyze and study the existing resources available on Internet Completed January 2019 Study the reference papers and experts in NMT and analyze their opinions Completed February 2019 Do same as January, concentrate more on the state-of-the-art practices Completed Mar-Dec 2019 Parallel corpora generation September 2019 Data ingestion pipeline Initial draft prepared October 2019 Read existing papers on MT and write the summary Delegated Nov-Dec 2019 Parallel corpora generation and data cleaning In-progress <p>The parallel corpora generation data has been moved to Odia Wikimedia. There will be no further work unless we have achieved at least 10k (12k/10k achieved) parallel corpus.</p>"},{"location":"machine_translation/progress/#detailed-works-completed-in-december-2018","title":"Detailed works completed in December 2018","text":"<ul> <li>Found corpus worth around 27,000 English-Odia tab separated translation pairs  This needs work on the followings:<ul> <li>Some preprocessing need to be done.</li> <li>All translation not so accurate based on few manual review</li> <li>Spelling mistakes are there</li> </ul> </li> <li>After reading one article on preprocessing English-Hindi corpus for SMT, I got insight on the following preprocessing tasks need to be done at first:<ul> <li>Punctuation should NOT be removed</li> <li>English text should be lower cased as Odia does not have any upper case or lower case</li> <li>Spell normalization (Which is like Lemmatization) will have a greater impact</li> <li>The impact of mapping numbers with unique class labels is not very effective and can be left out.</li> <li>We need NER (Named Entity Recognition) words for which Transliteration is enough</li> <li>We need POS tag data to improve the accuracy also.</li> </ul> </li> <li>Moses MT tool is open source and ideal for developing SMT for our purpose.<ul> <li>It needs parallel corpus to train and build the model after that prediction it can do.</li> <li>Data curation as explained on above point will be crucial to train this kind of system.</li> <li>If no better option available, we will mostly go with this approach.</li> </ul> </li> <li>Other than this I got a good sense to keep an eye on Hybrid Machine Translation, which will be mixture of RBMT and SMT.</li> <li>There is not enough corpus available to go for NMT/SMT currently. We need to prepare a platform like Google translate community where we can manually create English-Odia phrase or word pairs. For this the following needed:<ul> <li>UI hosted somewhere on cloud which can handle at least 100 requests per minute</li> <li>A DB, also hosted in cloud to store the data provided by users</li> <li>Possible suggestions for new users to start translating</li> <li>Review mechanism on the translated terms, even with good intention grammatical or syntactical errors need to be validated</li> <li>This infrastructure need to be hosted somewhere on cloud and it should be absolutely free to use.</li> </ul> </li> <li>Unknown words handling : There will be definitely terms will come for which Odia words will not be present. These are some suggestions to handle unknown words:<ul> <li>Transliterate those words to Odia</li> <li>Using some other existing translation system, convert those words to Hindi/Sanskrit then transliterate those words to Odia. Because most of the words in between Hindi/Sanskrit and Odia are same and people can understand.</li> </ul> </li> </ul>"},{"location":"machine_translation/progress/#detailed-works-completed-in-january-2019","title":"Detailed works completed in January 2019","text":"<ul> <li>Last year has been pretty exciting to prepare this plan. Hope this year we will be able to deliver something useful for the community.</li> <li>Quality of data is highly needed. Therefore started preparing phrase parallel corpus and contributing simultaneously to both Google and Facebook. How ? Google translation community and Facebook Translate are recommending phrases to translate. The same phrases I am keeping a copy to myself.</li> <li>Microsoft has given an exciting resource to public to train their own Parallel Translation model that is also FREE. Details:<ul> <li>It needs more than 10,000 pairs of parallel sentence pairs</li> <li>It seems to be working on SMT</li> <li>Train, Testing, Deployment all pipeline have been there</li> </ul> </li> <li>First attempt I have tried this tool with GNOME translation pair I got. However, It got failed may be due to only 60 parallel sentences.</li> <li>In the meantime there seems to be many work going on with frequent papers in Unsupervised NMT for low resources languages, there has not been any significant usable work yet. However, I am keeping an eye on that too.</li> <li>The Translator Hub has been retired on May 2019. It has been migrated to Microsoft Custom translator. However, Odia language is unavailable currently. I have requested them to add it.</li> </ul>"},{"location":"machine_translation/progress/#detailed-works-completedongoing-in-february-2019","title":"Detailed works completed/ongoing in February 2019","text":"<ul> <li>Data review and preparation started.</li> <li>Around 550 pairs reviewed till now. We need at least 10,000 pairs by end of this month. Finding ways to automate.</li> </ul>"},{"location":"machine_translation/progress/#data-ingestion-pipeline","title":"Data Ingestion pipeline","text":"<ul> <li>A pipeline flow draft for data ingestion been created.  </li> </ul>"},{"location":"machine_translation/progress/#reading-papers","title":"Reading papers","text":""},{"location":"machine_translation/progress/#six-challenges-for-neural-machine-translation","title":"Six Challenges for Neural Machine Translation","text":"<ul> <li>There are six challenges in NMT</li> <li>Domain mismatch</li> <li>Amount of Training Data</li> <li>Rare Words</li> <li>Long sentences</li> <li>Word Alignment</li> <li>Beam Search</li> </ul>"},{"location":"machine_translation/progress/#new-pairs-addition","title":"New pairs addition","text":"<p>Bible pairs from OdiEnCorp1.0 has been added to the consolidated corpus.</p>"},{"location":"machine_translation/progress/#nmt-deployment","title":"NMT deployment","text":"<ul> <li>Microsoft Azure ML pricing (for basic version)<ul> <li>It does not have GPU support</li> <li>Create and publish custom modules in Azure ML designer is not there</li> </ul> </li> </ul>"},{"location":"machine_translation/progress/#impediments","title":"Impediments","text":"<ul> <li> Get at least 10,000 parallel open corpus for Odia language to begin with.</li> <li> Verification of the existing corpus badly needed.</li> <li> Moses does not run on Windows. Need an Ubuntu OS to test that.</li> <li> Need a cloud system to host manual translation API server and in future for online translation. Is it Microsoft ?</li> </ul>"},{"location":"machine_translation/progress/#referred-articleswebsites","title":"Referred articles/websites","text":"<ul> <li> <p>Apertium Wiki for Odia language</p> </li> <li> <p>Indic Languages Multilingual Parallel Corpus</p> </li> <li>The RGNLP Machine Translation Systems for WAT 2018</li> <li>Anuvadaksh- An online existing English-Odia translator</li> <li>Wordnet for Odia</li> <li>RBMT vs SMT</li> <li>Detail MT system analysis of Indic languages</li> <li>English-Punjabi parallel corpus creation</li> <li>Creating more corpus by breaking long sentences</li> </ul>"},{"location":"machine_translation/progress/#useful-open-source-libraries","title":"Useful Open source libraries","text":"<ul> <li>fast_align : Align the words between two parallel corpus</li> </ul>"},{"location":"machine_translation/progress/#prospective-data-corpus","title":"Prospective data corpus","text":"<p>These are few places where relevant data may be present, however getting the data is not straight forward.  </p> <ul> <li> <p>EMILLE Project :   The Oriya written corpus consists of data incorporated from the CIIL Corpus, originally gathered by the Institute of Applied Language Sciences, Bhubaneshwar (approximately 2,730,000 words).</p> </li> <li> <p>Gyan Nidhi-TDIL : Million pages\u2019 multilingual parallel text corpus in English and 11  Indian  languages  (Assamese,  Bengali,  Gujarati,  Hindi, Kannada,   Marathi,   Malayalam,   Oriya,   Punjabi,   Tamil   &amp; Telugu)  based  on  Unicode  encoding.  The  Gyan  Nidhi  corpus contains  the  text  in  the  form  of  books. In  these  books  there were  number of diagrams,  figures,  charts  and  other  special symbols. These are removed from the text by using automated and  manual  tools.  The  text  in  gyan  nidhi  is  in  the  form of paragraphs, that are converted into short sentences.</p> </li> <li> <p>A Gold Standard Odia Raw Text Corpus : Around 15, 88, 287 words are there in this corpus available to purchase.</p> </li> </ul> <p>\"In my dream of the 21st century for the State, I would have young men and women who put the interest of the State before them. They will have pride in themselves, confidence in themselves. They will not be at anybody\u2019s mercy, except their own selves. By their brains, intelligence and capacity, they will recapture the history of Kalinga.\" - Biju Pattnaik</p>"},{"location":"machine_translation/sentence_alignment/","title":"Sentence Alignment","text":"<p>The biggest Issues I am receiving to collect the parallel corpus is sentences alignment.</p>"},{"location":"machine_translation/sentence_alignment/#issues","title":"Issues","text":"<ol> <li>The sentences are in an unordered format. Where the first sentence from one language might match with third sentence with second language.</li> <li>second one</li> </ol>"},{"location":"machine_translation/sentence_alignment/#possible-solutions","title":"Possible Solutions","text":"<ol> <li>Using a set of pre-translated words pairs to match among the sentence pairs.</li> <li>Using numbers among the pairs to match.</li> <li>Crawl the entire purnachandra bhashakosh and prepare the dictionary.</li> </ol>"},{"location":"machine_translation/tweets_extraction/","title":"Tweets Extraction","text":""},{"location":"machine_translation/tweets_extraction/#requirements","title":"Requirements","text":""},{"location":"machine_translation/tweets_extraction/#must-have","title":"Must have","text":"<ul> <li>For a specific hashtag, extract the tweets.</li> <li>Remove personalization from the tweets</li> <li>Detect Odia language content</li> <li>Find language boundary in the tweet</li> <li>Prepare English and Odia pairs</li> <li>Converted the pairs into structured format</li> <li>Put the pairs in a DB or file.</li> </ul>"},{"location":"machine_translation/tweets_extraction/#good-to-have","title":"Good to have","text":"<ul> <li>Real time streaming tweet extraction</li> </ul>"},{"location":"machine_translation/ui_for_corpus_generation/","title":"Problem Statement","text":"<p>There is a need of UI for easy corpus collection. Which will lead to translation of English to Odia text and later Odia to English.</p>"},{"location":"machine_translation/ui_for_corpus_generation/#why-a-person-should-contribute-here","title":"Why a person should contribute here?","text":"<p>Along with corpus collection, it should have the following features:</p> <ul> <li>Points/Rank generation</li> <li>Gift coupons and other annual prize distribution</li> <li>Smooth synchronization with OAuth2 social media tokens</li> </ul> <p>The UI will have two tabs:</p> <ol> <li>Add<ol> <li>Word</li> <li>Phrase</li> <li>Sentence</li> </ol> </li> <li>Review<ol> <li>Word</li> <li>Phrase</li> <li>Sentence</li> <li>Paragraph</li> </ol> </li> </ol>"},{"location":"machine_translation/ui_for_corpus_generation/#solution","title":"Solution","text":"<ul> <li>https://mte2o-review.herokuapp.com building is in progress.</li> </ul>"},{"location":"newsfeed/newsfeed/","title":"Updates","text":""},{"location":"newsfeed/newsfeed/#2021","title":"2021","text":""},{"location":"newsfeed/newsfeed/#october","title":"October","text":"<ul> <li>6th Oct</li> <li>The English Odia dictionary datasets cleaned. </li> <li>Which got reduced by around 7000 word pairs. Currently, we have 2,08,046 number of parallel English and Odia word/phrase pairs.</li> <li>The dataset can be found in the Kaggle. This will be soon added into the OpenOdia Python library.</li> <li>1st Oct</li> <li>OpenOdia project reference added.</li> </ul>"},{"location":"newsfeed/newsfeed/#september","title":"September","text":"<ul> <li>6th Sep<ul> <li>More data added to the Possible projects documentation page.</li> <li>GitHub Actions enabled to automatically deploy to website in case of push to <code>master</code> branch.</li> </ul> </li> </ul>"},{"location":"newsfeed/newsfeed/#august","title":"August","text":"<ul> <li> <p>15th Aug:</p> <ul> <li>English-Odia enabled in the dictionary project.</li> <li>More data added to the Dictionary project documentation.</li> </ul> </li> <li> <p>8th Aug:     This website has been upgraded.</p> <ul> <li>Tabs moved from left to top.</li> <li>mte2o.com domain pointed here.</li> <li>Both light and dark theme added.</li> <li>Logo and Favicon added.</li> </ul> </li> </ul>"},{"location":"newsfeed/newsfeed/#2020","title":"2020","text":""},{"location":"newsfeed/newsfeed/#october_1","title":"October","text":"<ul> <li>4th Oct: Work on Machine Translation has been been presented on Odias in ML conference.</li> <li>4th Oct: First blog post on the learning in Odias in ML added.</li> </ul>"},{"location":"newsfeed/newsfeed/#september_1","title":"September","text":"<ul> <li>11th Sep: Praharaj Dictionary data has been shared on Kaggle for public.</li> <li>11th Sep: Odia persons' name in Odia language corpus created on Kaggle.</li> <li>7th Sep: Links for English-Odia text parallel corpus resources added.</li> <li>2nd Sep: Initial commit to this repo with basic website structure setup.</li> </ul>"},{"location":"newsfeed/newsfeed/#august_1","title":"August","text":"<ul> <li>28th Aug: Blog post on \"India\u2019s Odia language added to Google and Microsoft translation services\"</li> </ul>"},{"location":"resources/audio/","title":"Audio resources","text":"<ul> <li>There are following open-source datasets currently available on Odia speech.</li> </ul> Dataset License Need to be cleaned? Estimated hours of audio duration Note Mozilla Common Voice CC-0 No 11hrs (out of which ~2hrs verified) Odia text to speech/Speech to text corpus. Our team actively contribute on this project. Odia Pronunciations CC-BY-SA-4.0 No 20,000+ words/phrases <p>To cite this resource list, please use:</p> <pre><code>@misc{OdiaNLP,\nauthor       = {Soumendra Kumar Sahoo},\ntitle        = {Audio resources by Odia NLP},\nhowpublished = {\\url{https://www.mte2o.com/}},\nyear         = {2021}\n}\n</code></pre>"},{"location":"resources/text/","title":"Text","text":"<p>This page contains probable open source resources found over internet on Odia text.</p>"},{"location":"resources/text/#monolingual-corpus","title":"Monolingual Corpus","text":"<ul> <li>For much NLP related tasks a large monolingual corpus is a necessity.</li> <li>As the name suggest a monolingual corpus stores text of only one language. In this case it will be of Odia.</li> <li>An existing monolingual corpus can be found in the IndicNLP corpus with 6.94 million sentences formed of 107 million tokens.</li> <li>Similarly, you can check this resource list by Dr. Shantipriya Parida for further monolingual corpus in Odia language </li> </ul>"},{"location":"resources/text/#parallel-corpus","title":"Parallel Corpus","text":"Dataset License Need to be cleaned? Estimated number of pairs Note Wikipedia Data dump CC BY-SA 3.0 Yes 2,00,000+ Pairs can be taken from the dumps. (Reference). OdiEnCorp 1.0 CC BY-NC-SA 4.0 Yes 1,00,000+ Mostly Bible data OdiaNLP corpus CC BY-NC 4.0 No 80,437 Generic En-Or pairs collected from many sources with multiple liceses. OdiEnCorp 2.0 CC BY-NC-SA 4.0 - 94,000+ Quality not checked. TDIL Freeware Yes 52,000+ Technical terms strings Press Information Bureau CC-BY-4.0 Yes 58,461 Sentences are aligned. Contains Mann Ki Baat pairs too. Mann ki baat CC-BY-4.0 Yes 38,359 High quality translation and much more. However, cleaning needs to be done. IndoWordnet CC BY-SA 4.0 - 30,000+ Corpus quality need to be checked. OPUS Multiple - 6,00,000+ Corpus contitute of the following corporas: Wikimedia, GNOME, Mozilla, etc. Samanantar CC BY-SA 4.0 - 10,00,000+ One of the best available corpus as of Dec 2021."},{"location":"resources/text/#odia-transliteration","title":"Odia Transliteration","text":"<ul> <li> <p>Odia Wikimedia community has developed an open-source unicode converter to transliterate between various languages to Odia language.</p> Languages supported <ul> <li>The following languages are supported now:<ul> <li>Ahamiya</li> <li>Bengali</li> <li>Santali</li> <li>Hindi</li> <li>Gujarati</li> <li>Roman and</li> <li>Urdu</li> </ul> </li> </ul> <ul> <li>Online demo</li> </ul> </li> </ul>"},{"location":"resources/text/#additional-resources","title":"Additional Resources","text":"<ul> <li>Odia-NLP-Resource-Catalog by Dr. Shantipriya Parida</li> <li>A Catalog of resources for Indian language NLP by AI4Bharat team</li> <li>OpenOdia : Tools for Odia language</li> </ul> <p>To cite this resource list, please use:</p> <pre><code>@misc{OdiaNLP,\nauthor       = {Soumendra Kumar Sahoo},\ntitle        = {Text resources by Odia NLP},\nhowpublished = {\\url{https://www.mte2o.com/}},\nyear         = {2021}\n}\n</code></pre>"},{"location":"resources/visual/","title":"Coming Soon...","text":"<p>This page will contain all the open source resources found over internet on Odia images or videos.</p>"}]}