# Possible projects

This list contains the proposals for the projects which can be started with Odia language. This list has been created keeping students, research scholars and hobbist in mind, who by little knowledge on this domain can learn and able to execute this project.

- [Monolingual corpus](#monolingual-corpus)
- [Language detector](#lang-detector)
- [Word tokenizer](#word-tokenizer)
- [Word2vec preparation](#word2vec)
- [Word similarity](#word-similarity)
- [Sentence tokenizer](#sent-tokenizer)
- [Random sentence generator](#sentence-generator)
- [Sentence similarity](#sentence-similarity)
- [Stemming](#stemming)
- [Synonym](#synonym)
- [Part of Speech tagging](#pos-tag)
- [Dependency parse tree creation](#dep-parse-tree)
- [Lemmatization](#lemmatization)
- [Sentiment analysis](#sentiment-analysis)
- [Text classification](#text-classification)
- [Co-reference resolution](#coref-resolution)
- [Word(s) based sentence creation](#word-based-sentence-generator)
- [Random quotation generator](#quote-generator)
- [Author specific Quotation generation](#auth-quote-generator)
- [Syntactically sentence correction](#sentence-correction)
- [Correct punctuation marks](#punct-correction)
- [Autocomplete](#autocomplete)
- [Spell corrector](#spell-corrector)
- [Sentence completion](#sentence-completion)
- [Automatic speech recognition (major project)](#speech-recognition)

## <a name="monolingual-corpus"></a> Monolingual corpus

- For many NLP related tasks a large [monolingual corpus](https://en.wikipedia.org/wiki/Text_corpus) is a necessity.
- As the name suggest a monolingual corpus (MC) stores text of only one language. In this case it will be of Odia.
- The MC mainly consists of full sentences.

### Odia Wikipedia as an MC provider

- The major provider of MC data can be found in Odia Wikipedia.
- The source code to crawl and extract Wikipedia data can be found [here](https://github.com/goru001/nlp-for-odia/tree/master/datasets-preparation)
- You can directly get the dump from the Wikimedia site itself, you can get data dump of both Wikisource (Odia books) and Wikipedia (Odia articles) from [this site](https://dumps.wikimedia.org/backup-index.html). Need to cleanup a bit as well.

### How-to do?

This is a data collection project.  

1. Extract the content from various websites with CC license by writing [web crawlers](https://en.wikipedia.org/wiki/Web_crawler) or taking wikipedia dump  
2. Clear the corpus.  
3. Prepare an Odia language sentences corpus.

## <a name="lang-detector"></a> Language detector

Given a text string, detect its language.
It should identify if Odia language text are given.

### Existing Algorithms

1. [Google language detector](https://github.com/CLD2Owners/cld2)

2.

## <a name="pos-tag"></a> Part of speech tagging

Given a sentence find out the part of speeches present on that sentence.  
Part of speech can be verb, noun, adjective, pronoun, preposition, etc.

## <a name="stemming"></a> Stemming

## <a name="lemmatization"></a> Lemmatization
